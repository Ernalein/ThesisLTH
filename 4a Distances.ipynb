{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6283b45",
   "metadata": {},
   "source": [
    "# Distances\n",
    "\n",
    "This notebook contains\n",
    "- necessary imports\n",
    "- distance measures for distributions\n",
    "- metfods for getting distances between conditions\n",
    "- boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f30c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67d2ee",
   "metadata": {},
   "source": [
    "## Distance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe34826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to turn arrays into discrete probability distributions\n",
    "\n",
    "def make_prob_distr(array1, array2):\n",
    "\n",
    "    union = []\n",
    "    union.extend(array1.flat)\n",
    "    union.extend(array2.flat)\n",
    "    unique_values = list(set(union))\n",
    "    unique_values.sort()\n",
    "\n",
    "    probdistr1 = []\n",
    "    probdistr2 = []\n",
    "    for v in unique_values:\n",
    "        probdistr1.append((array1 == v).sum()/len(array1.flat))\n",
    "        probdistr2.append((array2 == v).sum()/len(array2.flat))\n",
    "        \n",
    "    return probdistr1, probdistr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3288393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# 5 different Distance measure functions #\n",
    "##########################################\n",
    "\n",
    "\n",
    "# kullback leibler divergence\n",
    "def kl_divergence(array1, array2):\n",
    "    \n",
    "    def kl(array1, array2):\n",
    "        l_sum = [0.0]\n",
    "        for p,q in zip(array1, array2):\n",
    "            if p*q != 0.0:\n",
    "                l_sum.append(p * np.log(p / q))\n",
    "        return np.sum(l_sum)\n",
    "    \n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "    return kl(probdistr1, probdistr2)\n",
    "\n",
    "# Jensenâ€“Shannon divergence\n",
    "def js_divergence(array1, array2):\n",
    "    \n",
    "    def make_mixture_distribution(array1, array2):\n",
    "    \n",
    "        union = []\n",
    "        union.extend(array1.flat)\n",
    "        union.extend(array2.flat)\n",
    "        unique_values = list(set(union))\n",
    "        unique_values.sort()\n",
    "\n",
    "        probdistr = []\n",
    "        for v in unique_values:\n",
    "            probdistr1 = (array1 == v).sum() / len(array1.flat)\n",
    "            probdistr2 = (array2 == v).sum() / len(array2.flat)\n",
    "            probdistr.append(np.mean([probdistr1,probdistr2]))\n",
    "\n",
    "        return probdistr\n",
    "    \n",
    "    def kl(array1, array2):\n",
    "        l_sum = [0.0]\n",
    "        for p,q in zip(array1, array2):\n",
    "            if p*q != 0.0:\n",
    "                l_sum.append(p * np.log(p / q))\n",
    "        return np.sum(l_sum)\n",
    "    \n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "    mix_prob_distr = make_mixture_distribution(array1, array2)\n",
    "    js = 0.5*kl(probdistr1, mix_prob_distr) + 0.5*kl(probdistr2, mix_prob_distr)\n",
    "                          \n",
    "    return js\n",
    "\n",
    "# wasserstein distance or earths mover's distance\n",
    "def wasserstein_distance(array1, array2):\n",
    "    \n",
    "    union = []\n",
    "    union.extend(array1.flat)\n",
    "    union.extend(array2.flat)\n",
    "    unique_values = list(set(union))\n",
    "    unique_values.sort()\n",
    "\n",
    "    probdistr1 = []\n",
    "    probdistr2 = []\n",
    "    for v in unique_values:\n",
    "        probdistr1.append((array1 == v).sum()/len(array1.flat))\n",
    "        probdistr2.append((array2 == v).sum()/len(array2.flat))\n",
    "\n",
    "    return scipy.stats.wasserstein_distance(u_values=probdistr1, v_values=probdistr2, u_weights=unique_values, v_weights=unique_values)\n",
    "\n",
    "# bhattacharyya distance\n",
    "def bhattacharyya(array1, array2):\n",
    "    \n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "    bc = np.sum(np.sqrt(np.multiply(probdistr1, probdistr2)))\n",
    "    if bc == 0.0:\n",
    "        return np.nan\n",
    "    return - np.log(bc)\n",
    "\n",
    "\n",
    "# hellinger distance\n",
    "def hellinger_distance(array1, array2):\n",
    "    \n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "    bc = np.sum(np.sqrt(np.multiply(probdistr1, probdistr2)))                    \n",
    "    return np.sqrt(1-bc)\n",
    "\n",
    "def histogram_intersection(array1, array2):\n",
    "    \n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "    mins = []\n",
    "    for v1, v2 in zip(probdistr1, probdistr2):\n",
    "        mins.append(np.min([v1,v2]))\n",
    "    shared_ratio = np.sum(mins)/1\n",
    "    return 1 - shared_ratio\n",
    "\n",
    "def histogram_correlation(array1, array2):\n",
    "    \n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "    \n",
    "    mean1 = np.mean(probdistr1)\n",
    "    mean2 = np.mean(probdistr2)\n",
    "    numerator = []\n",
    "    denominator_a = []\n",
    "    denominator_b = []\n",
    "    \n",
    "    for v1,v2 in zip(probdistr1, probdistr2):\n",
    "        numerator.append((v1-mean1)*(v2-mean2))\n",
    "        denominator_a.append((v1-mean1)*(v1-mean1))\n",
    "        denominator_b.append((v2-mean2)*(v2-mean2))\n",
    "    numerator = np.sum(numerator)\n",
    "    denominator = np.sqrt(np.sum(denominator_a) * np.sum(denominator_b))\n",
    "    \n",
    "    # the correlation coefficient (between -1 and 1)\n",
    "    r = numerator/denominator\n",
    "    \n",
    "    # turning it into a distance measure\n",
    "    d = r*(-0.5) + 0.5\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff7c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total variation distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e1f1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi square statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64d3b4",
   "metadata": {},
   "source": [
    "## Extracting distances depending on conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb16a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_per_dataset = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c5ae4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting distances for a certain distance measure, layer and sign_distribution variable\n",
    "\n",
    "def distances_for_distributions(distance_measure, layer, variable):\n",
    "    \n",
    "    # extracting a sign distribution given a number and a dataset\n",
    "    def get_sign_distr(n, d_set):\n",
    "        sign_distr = None\n",
    "        if d_set == \"RSN\":\n",
    "            sign_distr = pd.read_csv(f'2b Sign distributions/RSN_{n}_sign_distr.csv')\n",
    "        else:\n",
    "            sign_distr = pd.read_csv(f'2b Sign distributions/{d_set}_IMP_{n}_sign_distr.csv')\n",
    "        return np.array(sign_distr[sign_distr[\"layer\"]==layer][[variable]])\n",
    "    \n",
    "    \n",
    "    distances = {\"CIFAR\":[], \"CINIC\":[], \"SVHN\":[], \"RSN\":[],\n",
    "                 \"RSN_CIFAR\":[], \"RSN_CINIC\":[], \"RSN_SVHN\":[],\n",
    "                 \"CIFAR_SVHN\":[], \"CINIC_CIFAR\":[], \"SVHN_CINIC\":[]}\n",
    "    \n",
    "    # taking pairs, but not both ways\n",
    "    for i in range(wts_per_dataset):\n",
    "        for j in range(wts_per_dataset):\n",
    "            \n",
    "            # comparing within conditions\n",
    "            if i < j:\n",
    "                for c in [\"CIFAR\", \"CINIC\", \"SVHN\", \"RSN\"]:\n",
    "                    distances[c].append(distance_measure(get_sign_distr(i, c),get_sign_distr(j, c)))\n",
    "                    \n",
    "            # comparing RSNs and WTs\n",
    "            for c in [\"CIFAR\", \"CINIC\", \"SVHN\"]:\n",
    "                distances[f\"RSN_{c}\"].append(distance_measure(get_sign_distr(i, \"RSN\"),get_sign_distr(j, c)))\n",
    "            \n",
    "            # comparing between datasets\n",
    "            distances[\"CIFAR_SVHN\"].append(distance_measure(get_sign_distr(i, \"CIFAR\"),get_sign_distr(j, \"SVHN\")))\n",
    "            distances[\"CINIC_CIFAR\"].append(distance_measure(get_sign_distr(i, \"CINIC\"),get_sign_distr(j, \"CIFAR\")))\n",
    "            distances[\"SVHN_CINIC\"].append(distance_measure(get_sign_distr(i, \"SVHN\"),get_sign_distr(j, \"CINIC\")))\n",
    "    \n",
    "    # add 4 collective distance conditions\n",
    "    distances[\"within_WTs\"] = distances[\"CIFAR\"]+distances[\"CINIC\"]+distances[\"SVHN\"]\n",
    "    distances[\"within_conditions\"] = distances[\"CIFAR\"]+distances[\"CINIC\"]+distances[\"SVHN\"]+distances[\"RSN\"]\n",
    "    distances[\"between_WTs_and_RSN\"] = distances[\"RSN_CIFAR\"]+distances[\"RSN_CINIC\"]+distances[\"RSN_SVHN\"]\n",
    "    distances[\"between_WT_datasets\"] = distances[\"CIFAR_SVHN\"]+distances[\"CINIC_CIFAR\"]+distances[\"SVHN_CINIC\"]\n",
    "            \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfb45d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the different plot parameters:\n",
    "\n",
    "distance_measures = [kl_divergence,\n",
    "                     js_divergence, \n",
    "                     wasserstein_distance, \n",
    "                     hellinger_distance, \n",
    "                     bhattacharyya, \n",
    "                     histogram_correlation, \n",
    "                     histogram_intersection]\n",
    "\n",
    "distance_measure_names = [\"kl_divergence\",\n",
    "                          \"js_divergence\", \n",
    "                          \"wasserstein_distance\",  \n",
    "                          \"hellinger_distance\", \n",
    "                          \"bhattacharyya\", \n",
    "                          \"histogram_correlation\", \n",
    "                          \"histogram_intersection\"]\n",
    "\n",
    "distances_short = [\"kl\",\"jsd\", \"wsd\",  \"hd\", \"bd\", \"hc\", \"hi\"]\n",
    "\n",
    "variables = [\"prune_rate_in\", \"prune_rate_out\", \"sign_rate_in\", \"sign_rate_out\"]\n",
    "layers = [\"dense1\", \"dense2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89a4e84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \r"
     ]
    }
   ],
   "source": [
    "# store all the distances\n",
    "\n",
    "def store_all_distances():\n",
    "\n",
    "    for d_measure, d_measure_name in tqdm(zip(distance_measures, distance_measure_names), leave=False, desc=\"distance_measures\"):\n",
    "        for layer in layers:\n",
    "            for variable in variables:\n",
    "            \n",
    "                # get distance\n",
    "                distances = distances_for_distributions(d_measure, layer, variable)\n",
    "\n",
    "                # store distance\n",
    "                myFile = open(f'4b Distances/{layer}_{variable}_{d_measure_name}_distances.txt', 'w')\n",
    "                myFile.write(str(distances))\n",
    "                myFile.close()\n",
    "\n",
    "store_all_distances()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e781fb",
   "metadata": {},
   "source": [
    "## Collecting mean and std of all distanc conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc73fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate std and mean for each condition and collect in a giant dataframe\n",
    "\n",
    "def get_mean_std_distances():\n",
    "\n",
    "    distance_statistics = pd.DataFrame()\n",
    "\n",
    "    for d_measure, d_measure_name in tqdm(zip(distance_measures, distance_measure_names), leave=False, desc=\"distance_measures\"):\n",
    "        for layer in layers:\n",
    "            for variable in variables:\n",
    "\n",
    "                # get distances for all conditions\n",
    "                distances = distances_for_distributions(d_measure, layer, variable)\n",
    "\n",
    "                # iterate thorugh all conditions and collect their mean and std in a dataframe\n",
    "                new_distance_statistics = pd.DataFrame()\n",
    "                for c_name, d in distances.items():\n",
    "                    dic = {}\n",
    "\n",
    "                    dic[\"mean\"] = np.mean(d)\n",
    "                    dic[\"std\"] = np.std(d)\n",
    "                    dic[\"layer\"] = layer\n",
    "                    dic[\"variable\"] = variable\n",
    "                    dic[\"distance_measure\"] = d_measure_name\n",
    "\n",
    "                    # add new row as dataframe to statistics\n",
    "                    df = pd.DataFrame(data = dic, index = [c_name])\n",
    "                    new_distance_statistics = pd.concat([new_distance_statistics, df], axis=0)\n",
    "\n",
    "                # add collected statistics do big data frame\n",
    "                distance_statistics = pd.concat([distance_statistics, new_distance_statistics], axis=0)\n",
    "                \n",
    "    return distance_statistics     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b524ea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# get distance statistics\n",
    "distance_statistics = get_mean_std_distances()\n",
    "\n",
    "# store distances statistics in a file\n",
    "distance_statistics.to_csv(f'4b Distances/mean_std_distances.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592d2a6",
   "metadata": {},
   "source": [
    "## Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c130693",
   "metadata": {},
   "source": [
    "Hypothesis:  \n",
    "\n",
    "1. Distances within WTs are smaller than between WTs and RSNs\n",
    "2. Distances within RSNs are smaller than between WTs and RSNs\n",
    "3. Distances within WTs are smaller than between WT datasts\n",
    "4. Distances between WT datasets are smaller than between WTs and RSNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "617c14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_pairs = [(\"within_WTs\",\"between_WTs_and_RSN\"),(\"RSN\",\"between_WTs_and_RSN\"),(\"within_WTs\",\"between_WT_datasets\"),(\"between_WT_datasets\",\"between_WTs_and_RSN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f90a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_t_tests():\n",
    "    \n",
    "    t_test_results = pd.DataFrame()\n",
    "\n",
    "    for d_measure, d_measure_name in tqdm(zip(distance_measures, distance_measure_names), leave=False, desc=\"distance_measures\"):\n",
    "        for layer in layers:\n",
    "            for variable in variables:\n",
    "\n",
    "                new_results = pd.DataFrame()\n",
    "\n",
    "                # get distances for all conditions\n",
    "                distances = distances_for_distributions(d_measure, layer, variable)\n",
    "\n",
    "                # iterate thorugh important conditions and perform corresponding hypothesis tests\n",
    "\n",
    "                # iterate thorugh hypotheses\n",
    "                for (d1,d2), h in zip(hypothesis_pairs, range(1,5)):\n",
    "\n",
    "                    # perform one sided two sample Welchâ€™s t-test\n",
    "                    h_results = scipy.stats.ttest_ind(distances[d1], distances[d2], alternative=\"less\", equal_var=False)\n",
    "                    p_value = h_results.pvalue\n",
    "\n",
    "                    # add test results as row in dictionary\n",
    "                    dic = {}\n",
    "                    dic[\"hypothesis\"] = f\"H{h}\"\n",
    "                    dic[\"accepted\"] = p_value<=0.05\n",
    "                    dic[\"p-value\"] = p_value\n",
    "                    dic[\"layer\"] = layer\n",
    "                    dic[\"variable\"] = variable\n",
    "                    dic[\"distance_measure\"] = d_measure_name\n",
    "\n",
    "                    # add row to datframe\n",
    "                    df = pd.DataFrame(data = dic, index = [h])\n",
    "                    new_results = pd.concat([new_results, df], axis=0)\n",
    "\n",
    "                # add collected statistics do big data frame\n",
    "                t_test_results = pd.concat([t_test_results, new_results], axis=0)\n",
    "                \n",
    "    return t_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72a77660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# get t_test_results\n",
    "significance_test_results = do_t_tests()\n",
    "\n",
    "# store t_test_results in a file\n",
    "significance_test_results.to_csv(f'4b Distances/significance_test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6a963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
