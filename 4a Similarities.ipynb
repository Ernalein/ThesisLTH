{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6283b45",
   "metadata": {},
   "source": [
    "# Similariteis\n",
    "\n",
    "\n",
    "This notebook contains\n",
    "- necessary imports\n",
    "- similarity measures for distributions\n",
    "- metfods for getting similarity scores between conditions\n",
    "- boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f30c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import ast\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67d2ee",
   "metadata": {},
   "source": [
    "## Similarity measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe34826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to turn arrays into discrete probability distributions\n",
    "\n",
    "def make_prob_distr(array1, array2):\n",
    "\n",
    "    union = []\n",
    "    union.extend(array1.flat)\n",
    "    union.extend(array2.flat)\n",
    "    unique_values = list(set(union))\n",
    "    unique_values.sort()\n",
    "\n",
    "    probdistr1 = []\n",
    "    probdistr2 = []\n",
    "    for v in unique_values:\n",
    "        probdistr1.append((array1 == v).sum()/len(array1.flat))\n",
    "        probdistr2.append((array2 == v).sum()/len(array2.flat))\n",
    "        \n",
    "    return probdistr1, probdistr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3288393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# different Similarity measure functions #\n",
    "##########################################\n",
    "\n",
    "\n",
    "# kullback leibler divergence\n",
    "def kl_divergence(array1, array2):\n",
    "    \n",
    "    def kl(array1, array2):\n",
    "        l_sum = [0.0]\n",
    "        for p,q in zip(array1, array2):\n",
    "            if p*q != 0.0:\n",
    "                l_sum.append(p * np.log(p / q))\n",
    "        return np.sum(l_sum)\n",
    "    \n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "    return -1 * kl(probdistr1, probdistr2)\n",
    "\n",
    "# Jensen–Shannon divergence\n",
    "def js_divergence(array1, array2):\n",
    "    \n",
    "    def make_mixture_distribution(array1, array2):\n",
    "    \n",
    "        union = []\n",
    "        union.extend(array1.flat)\n",
    "        union.extend(array2.flat)\n",
    "        unique_values = list(set(union))\n",
    "        unique_values.sort()\n",
    "\n",
    "        probdistr = []\n",
    "        for v in unique_values:\n",
    "            probdistr1 = (array1 == v).sum() / len(array1.flat)\n",
    "            probdistr2 = (array2 == v).sum() / len(array2.flat)\n",
    "            probdistr.append(np.mean([probdistr1,probdistr2]))\n",
    "\n",
    "        return probdistr\n",
    "    \n",
    "    def kl(array1, array2):\n",
    "        l_sum = [0.0]\n",
    "        for p,q in zip(array1, array2):\n",
    "            if p*q != 0.0:\n",
    "                l_sum.append(p * np.log(p / q))\n",
    "        return np.sum(l_sum)\n",
    "    \n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "    mix_prob_distr = make_mixture_distribution(array1, array2)\n",
    "    js = 0.5*kl(probdistr1, mix_prob_distr) + 0.5*kl(probdistr2, mix_prob_distr)\n",
    "                          \n",
    "    return -1 * js\n",
    "\n",
    "# wasserstein distance or earths mover's distance\n",
    "def wasserstein_distance(array1, array2):\n",
    "    \n",
    "    union = []\n",
    "    union.extend(array1.flat)\n",
    "    union.extend(array2.flat)\n",
    "    unique_values = list(set(union))\n",
    "    unique_values.sort()\n",
    "\n",
    "    probdistr1 = []\n",
    "    probdistr2 = []\n",
    "    for v in unique_values:\n",
    "        probdistr1.append((array1 == v).sum()/len(array1.flat))\n",
    "        probdistr2.append((array2 == v).sum()/len(array2.flat))\n",
    "\n",
    "    return -1 * scipy.stats.wasserstein_distance(u_values=probdistr1, v_values=probdistr2, u_weights=unique_values, v_weights=unique_values)\n",
    "\n",
    "# bhattacharyya distance\n",
    "def bhattacharyya(array1, array2):\n",
    "    \n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "    bc = np.sum(np.sqrt(np.multiply(probdistr1, probdistr2)))\n",
    "    if bc == 0.0:\n",
    "        return np.nan\n",
    "    return -1 * (- np.log(bc))\n",
    "\n",
    "\n",
    "# hellinger distance\n",
    "def hellinger_distance(array1, array2):\n",
    "    \n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "    bc = np.sum(np.sqrt(np.multiply(probdistr1, probdistr2)))                    \n",
    "    return -1 * np.sqrt(1-bc)\n",
    "\n",
    "# histogram_intersection\n",
    "def histogram_intersection(array1, array2):\n",
    "    \n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "    mins = []\n",
    "    for v1, v2 in zip(probdistr1, probdistr2):\n",
    "        mins.append(np.min([v1,v2]))\n",
    "    shared_ratio = np.sum(mins)/1\n",
    "    return 1 - shared_ratio\n",
    "\n",
    "# histogram_correlation\n",
    "def histogram_correlation(array1, array2):\n",
    "    \n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "    \n",
    "    mean1 = np.mean(probdistr1)\n",
    "    mean2 = np.mean(probdistr2)\n",
    "    numerator = []\n",
    "    denominator_a = []\n",
    "    denominator_b = []\n",
    "    \n",
    "    for v1,v2 in zip(probdistr1, probdistr2):\n",
    "        numerator.append((v1-mean1)*(v2-mean2))\n",
    "        denominator_a.append((v1-mean1)*(v1-mean1))\n",
    "        denominator_b.append((v2-mean2)*(v2-mean2))\n",
    "    numerator = np.sum(numerator)\n",
    "    denominator = np.sqrt(np.sum(denominator_a) * np.sum(denominator_b))\n",
    "    \n",
    "    # the correlation coefficient (between -1 and 1)\n",
    "    r = numerator/denominator\n",
    "    \n",
    "    return r\n",
    "\n",
    "# total variation distance\n",
    "def total_variation(array1, array2):\n",
    "\n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "    tv = np.max(np.abs(np.array(probdistr1) - np.array(probdistr2)))\n",
    "\n",
    "    return -1 * tv\n",
    "\n",
    "# chi square statistics\n",
    "def chi2_distance(array1, array2):\n",
    "\n",
    "    probdistr1, probdistr2 = make_prob_distr(array1, array2)\n",
    "\n",
    "    numerator = (np.array(probdistr1) - np.array(probdistr2)) **2\n",
    "    denominator = np.array(probdistr1) + np.array(probdistr2)\n",
    "    chi = 0.5 * np.sum(numerator/denominator)\n",
    " \n",
    "    return -1 * chi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64d3b4",
   "metadata": {},
   "source": [
    "## Extracting similarities depending on conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb16a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_per_dataset = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5ae4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting similarities for a certain similarity measure, layer and sign_distribution variable\n",
    "\n",
    "def similarities_for_distributions(similarity_measure, layer, variable):\n",
    "    \n",
    "    # extracting a sign distribution given a number and a dataset\n",
    "    def get_sign_distr(n, d_set):\n",
    "        sign_distr = None\n",
    "        if d_set == \"RSN\":\n",
    "            sign_distr = pd.read_csv(f'2b Sign distributions/RSN_{n}_sign_distr.csv')\n",
    "        else:\n",
    "            sign_distr = pd.read_csv(f'2b Sign distributions/{d_set}_IMP_{n}_sign_distr.csv')\n",
    "        return np.array(sign_distr[sign_distr[\"layer\"]==layer][[variable]])\n",
    "    \n",
    "    \n",
    "    similarities = {\"CIFAR\":[], \"CINIC\":[], \"SVHN\":[], \"RSN\":[],\n",
    "                 \"RSN_CIFAR\":[], \"RSN_CINIC\":[], \"RSN_SVHN\":[],\n",
    "                 \"CIFAR_SVHN\":[], \"CINIC_CIFAR\":[], \"SVHN_CINIC\":[]}\n",
    "    \n",
    "    # taking pairs, but not both ways\n",
    "    for i in range(wts_per_dataset):\n",
    "        for j in range(wts_per_dataset):\n",
    "            \n",
    "            # comparing within conditions\n",
    "            if i < j:\n",
    "                for c in [\"CIFAR\", \"CINIC\", \"SVHN\", \"RSN\"]:\n",
    "                    similarities[c].append(similarity_measure(get_sign_distr(i, c),get_sign_distr(j, c)))\n",
    "                    \n",
    "            # comparing RSNs and WTs\n",
    "            for c in [\"CIFAR\", \"CINIC\", \"SVHN\"]:\n",
    "                similarities[f\"RSN_{c}\"].append(similarity_measure(get_sign_distr(i, \"RSN\"),get_sign_distr(j, c)))\n",
    "            \n",
    "            # comparing between datasets\n",
    "            similarities[\"CIFAR_SVHN\"].append(similarity_measure(get_sign_distr(i, \"CIFAR\"),get_sign_distr(j, \"SVHN\")))\n",
    "            similarities[\"CINIC_CIFAR\"].append(similarity_measure(get_sign_distr(i, \"CINIC\"),get_sign_distr(j, \"CIFAR\")))\n",
    "            similarities[\"SVHN_CINIC\"].append(similarity_measure(get_sign_distr(i, \"SVHN\"),get_sign_distr(j, \"CINIC\")))\n",
    "    \n",
    "    # add 4 collective distance conditions\n",
    "    similarities[\"within_WTs\"] = similarities[\"CIFAR\"]+similarities[\"CINIC\"]+similarities[\"SVHN\"]\n",
    "    similarities[\"within_conditions\"] = similarities[\"CIFAR\"]+similarities[\"CINIC\"]+similarities[\"SVHN\"]+similarities[\"RSN\"]\n",
    "    similarities[\"between_WTs_and_RSN\"] = similarities[\"RSN_CIFAR\"]+similarities[\"RSN_CINIC\"]+similarities[\"RSN_SVHN\"]\n",
    "    similarities[\"between_WT_datasets\"] = similarities[\"CIFAR_SVHN\"]+similarities[\"CINIC_CIFAR\"]+similarities[\"SVHN_CINIC\"]\n",
    "            \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfb45d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the different plot parameters:\n",
    "\n",
    "similarity_measures = [kl_divergence,\n",
    "                     js_divergence, \n",
    "                     wasserstein_distance, \n",
    "                     hellinger_distance, \n",
    "                     bhattacharyya, \n",
    "                     histogram_correlation, \n",
    "                     histogram_intersection]\n",
    "\n",
    "similarity_measure_names = [\"kl_divergence\",\n",
    "                          \"js_divergence\", \n",
    "                          \"wasserstein_distance\",  \n",
    "                          \"hellinger_distance\", \n",
    "                          \"bhattacharyya\", \n",
    "                          \"histogram_correlation\", \n",
    "                          \"histogram_intersection\"]\n",
    "\n",
    "similarity_short = [\"kl\",\"jsd\", \"wsd\",  \"hd\", \"bd\", \"hc\", \"hi\"]\n",
    "\n",
    "variables = [\"prune_rate_in\", \"prune_rate_out\", \"sign_rate_in\", \"sign_rate_out\"]\n",
    "layers = [\"dense1\", \"dense2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a67c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a subset of the similarity measures\n",
    "\n",
    "similarity_measures = [js_divergence, \n",
    "                     wasserstein_distance, \n",
    "                     hellinger_distance,\n",
    "                     total_variation,\n",
    "                     chi2_distance,\n",
    "                     histogram_correlation, \n",
    "                     histogram_intersection]\n",
    "\n",
    "similarity_measure_names = [\"js_divergence\", \n",
    "                          \"wasserstein_distance\",  \n",
    "                          \"hellinger_distance\",\n",
    "                          \"total_variation\",\n",
    "                          \"chi2_distance\", \n",
    "                          \"histogram_correlation\", \n",
    "                          \"histogram_intersection\"]\n",
    "\n",
    "similarity_short = [\"jsd\", \"wsd\",  \"hd\", \"tv\", \"chi2\", \"hc\", \"hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a4e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the similarities\n",
    "\n",
    "def store_all_similarities():\n",
    "\n",
    "    for s_measure, s_measure_name, s_short in tqdm(zip(similarity_measures, similarity_measure_names, similarity_short), leave=False, desc=\"similarity_measures\"):\n",
    "        for layer in layers:\n",
    "            for variable in variables:\n",
    "            \n",
    "                # get distance\n",
    "                similarities = similarities_for_distributions(s_measure, layer, variable)\n",
    "\n",
    "                # store distance\n",
    "                myFile = open(f'4b Similarities/{s_measure_name}/{layer}_{variable}_{s_short}_similarities.txt', 'w')\n",
    "                myFile.write(str(similarities))\n",
    "                myFile.close()\n",
    "\n",
    "#store_all_similarities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad9785c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(s_measure_name, s_measure_short, layer, variable):\n",
    "                \n",
    "    myFile = open(f'4b Similarities/{s_measure_name}/{layer}_{variable}_{s_measure_short}_similarities.txt', 'r')\n",
    "    similarity = myFile.read()\n",
    "    similarity = ast.literal_eval(similarity) \n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e781fb",
   "metadata": {},
   "source": [
    "## Collecting mean and std of all distanc conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc73fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate std and mean for each condition and collect in a giant dataframe\n",
    "\n",
    "def get_mean_std_similarities():\n",
    "\n",
    "    similarity_statistics = pd.DataFrame()\n",
    "\n",
    "    for s_measure, s_measure_name, s_measure_short in tqdm(zip(similarity_measures, similarity_measure_names, similarity_short), leave=False, desc=\"similarity_measures\"):\n",
    "        for layer in layers:\n",
    "            for variable in variables:\n",
    "\n",
    "                # get similariteis for all conditions\n",
    "                #similarities = similarities_for_distributions(s_measure, layer, variable)\n",
    "                similarities = get_similarity(s_measure_name, s_measure_short, layer, variable)\n",
    "\n",
    "                # iterate thorugh all conditions and collect their mean and std in a dataframe\n",
    "                new_similarity_statistics = pd.DataFrame()\n",
    "                for c_name, s in similarities.items():\n",
    "                    dic = {}\n",
    "\n",
    "                    dic[\"mean\"] = np.mean(s)\n",
    "                    dic[\"std\"] = np.std(s)\n",
    "                    dic[\"layer\"] = layer\n",
    "                    dic[\"variable\"] = variable\n",
    "                    dic[\"similarity_measure\"] = s_measure_name\n",
    "\n",
    "                    # add new row as dataframe to statistics\n",
    "                    df = pd.DataFrame(data = dic, index = [c_name])\n",
    "                    new_similarity_statistics = pd.concat([new_similarity_statistics, df], axis=0)\n",
    "\n",
    "                # add collected statistics do big data frame\n",
    "                similarity_statistics = pd.concat([similarity_statistics, new_similarity_statistics], axis=0)\n",
    "                \n",
    "    return similarity_statistics     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b524ea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                           \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# get similarity statistics\n",
    "similarity_statistics = get_mean_std_similarities()\n",
    "\n",
    "# store similarity statistics in a file\n",
    "similarity_statistics.to_csv(f'4b Similarities/mean_std_similarities.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592d2a6",
   "metadata": {},
   "source": [
    "## Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c130693",
   "metadata": {},
   "source": [
    "Hypothesis:  \n",
    "\n",
    "1. WTs are more similar with each other than WTs with RSNs.\n",
    "2. RSNs are more similar with each other than WTs with RSNs.\n",
    "3. WTs of the same dataset are more similar with each other than WTs of different datasets.\n",
    "4. WTs of different datasets are more similar with each than WTs with RSNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "617c14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_pairs = [(\"within_WTs\",\"between_WTs_and_RSN\"),(\"RSN\",\"between_WTs_and_RSN\"),(\"within_WTs\",\"between_WT_datasets\"),(\"between_WT_datasets\",\"between_WTs_and_RSN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f90a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_t_tests():\n",
    "    \n",
    "    t_test_results = pd.DataFrame()\n",
    "\n",
    "    for s_measure, s_measure_name, s_measure_short in tqdm(zip(similarity_measures, similarity_measure_names, similarity_short), leave=False, desc=\"similarity_measures\"):\n",
    "        for layer in layers:\n",
    "            for variable in variables:\n",
    "\n",
    "                new_results = pd.DataFrame()\n",
    "\n",
    "                # get similarities for all conditions\n",
    "                #similarities = similarities_for_distributions(s_measure, layer, variable)\n",
    "                similarities = get_similarity(s_measure_name, s_measure_short, layer, variable)\n",
    "\n",
    "                # iterate thorugh important conditions and perform corresponding hypothesis tests\n",
    "\n",
    "                # iterate thorugh hypotheses\n",
    "                for (s1,s2), h in zip(hypothesis_pairs, range(1,5)):\n",
    "\n",
    "                    # perform one sided two sample Welch’s t-test\n",
    "                    h_results = scipy.stats.ttest_ind(similarities[s1], similarities[s2], alternative=\"greater\", equal_var=False)\n",
    "                    p_value = h_results.pvalue\n",
    "\n",
    "                    # add test results as row in dictionary\n",
    "                    dic = {}\n",
    "                    dic[\"hypothesis\"] = f\"H{h}\"\n",
    "                    dic[\"accepted\"] = p_value<=0.05\n",
    "                    dic[\"p-value\"] = p_value\n",
    "                    dic[\"layer\"] = layer\n",
    "                    dic[\"variable\"] = variable\n",
    "                    dic[\"similarity_measure\"] = s_measure_name\n",
    "\n",
    "                    # add row to datframe\n",
    "                    df = pd.DataFrame(data = dic, index = [h])\n",
    "                    new_results = pd.concat([new_results, df], axis=0)\n",
    "\n",
    "                # add collected statistics do big data frame\n",
    "                t_test_results = pd.concat([t_test_results, new_results], axis=0)\n",
    "                \n",
    "    return t_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72a77660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                           \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# get t_test_results\n",
    "significance_test_results = do_t_tests()\n",
    "\n",
    "# store t_test_results in a file\n",
    "significance_test_results.to_csv(f'4b Similarities/significance_test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6a963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
