{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c50ec5",
   "metadata": {},
   "source": [
    "# Attribution methods\n",
    "\n",
    "In this notebook I perform attribution methods with the Wts and check for correlation with the sign distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa14221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 11:14:51.954934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis_no_gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "from cnn_architecture import CNN2Model\n",
    "from utils import *\n",
    "from load_datasets import load_and_prep_dataset\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00721f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2Model_adapted(tf.keras.Model):\n",
    "    \n",
    "    # basic\n",
    "    def __init__(self):\n",
    "        super(CNN2Model_adapted, self).__init__()\n",
    "        \n",
    "        # set biases to a value that is not exactly 0.0, so they don't get handled like pruned values\n",
    "        self.bias_in = tf.keras.initializers.Constant(value=0.0000000001)\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=3,activation=\"relu\", padding=\"same\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batchsize,32,32,64]\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3,activation=\"relu\", padding=\"same\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batchsize,32,32,64]\n",
    "        self.maxpool = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2, 2),input_shape=(32, 32, 64)) # [batchsize,16,16,64]\n",
    "        self.flatten = tf.keras.layers.Flatten() # [batch_size,16384]\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation=\"relu\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batch_size,256]\n",
    "        self.dense2 = tf.keras.layers.Dense(256, activation=\"relu\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batch_size,256]\n",
    "        self.dense3 = tf.keras.layers.Dense(10, activation=\"softmax\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batch_size,256]\n",
    "\n",
    "    #@tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        dense1_input = self.flatten(x)\n",
    "        dense1_out = self.dense1(dense1_input)\n",
    "        dense2_out = self.dense2(dense1_out)\n",
    "        dense3_out = self.dense3(dense2_out)\n",
    "        return dense3_out, dense1_out[0], dense2_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832c1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get contribution values for neurons\n",
    "def contribution_scores(test, model):\n",
    "\n",
    "    # hyperparameters\n",
    "    loss_function= tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    # initializeing contribution scores\n",
    "    c_score_1 = np.zeros(256)\n",
    "    c_score_2 = np.zeros(256)\n",
    "\n",
    "    # iterate through testing set and get gradients for each weight\n",
    "    first_round = True\n",
    "    for x, t in tqdm(test):\n",
    "\n",
    "        # get gradients \n",
    "        with tf.GradientTape() as tape:\n",
    "            pred, dense1_out, dense2_out = model(x)\n",
    "            loss = loss_function(t, pred)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "        # set gradients to zero for all pruned weights\n",
    "        new_gradients = []\n",
    "        for gradient_matrix, variables in zip(gradients, model.trainable_variables):\n",
    "            gradient_matrix = tf.where(variables == 0.0, 0.0, gradient_matrix)\n",
    "            new_gradients.append(gradient_matrix)\n",
    "        gradients_dense2 = new_gradients[6]\n",
    "        gradients_dense3 = new_gradients[8]\n",
    "\n",
    "        # compute c score for each neuron\n",
    "        for i in range(256):\n",
    "            c_score_1[i] = c_score_1[i] + np.sum(np.abs(gradients_dense2[i]*dense1_out[i]))\n",
    "            c_score_2[i] = c_score_2[i] + np.sum(np.abs(gradients_dense3[i]*dense2_out[i]))\n",
    "\n",
    "    # normalize c_scores to values between 0 and 1\n",
    "    c_score_1 = c_score_1 / np.sum(c_score_1)\n",
    "    c_score_2 = c_score_2 / np.sum(c_score_2)\n",
    "    \n",
    "    return  c_score_1,c_score_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edacb855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_computing_c_scores():\n",
    "\n",
    "    # make a model to load the weights into\n",
    "    train_dataset, test_dataset = load_and_prep_dataset(\"CIFAR\", batch_size=1, shuffle_size=512)\n",
    "    model = CNN2Model_adapted()\n",
    "    model(list(train_dataset)[0][0])\n",
    "    weights_wt = model.get_weights()\n",
    "    #print(weights_wt[0][0][0][0])\n",
    "    \n",
    "    # get WT weights\n",
    "    #model.load_weights(f\"1b WTs/old_format/WT_CIFAR_IMP_0\") # does not work for thesis env, works for no_gpu, does not work for gpu\n",
    "    #model.load_weights(f\"1b WTs/new_format/WT_CIFAR_IMP_0.weights.h5\") # does not work for thesis env, works for no_gpu, does not work for gpu\n",
    "    model.load_weights(f\"1b WTs/h5_format/WT_CIFAR_IMP_0.h5\") # does work for thesis env, works for no_gpu, works for gpu     \n",
    "    weights_wt = model.get_weights()\n",
    "    #print(weights_wt[0][0][0][0])\n",
    "\n",
    "    # plot distirbution of contribution values\n",
    "    for c_score in contribution_scores(test_dataset, model):\n",
    "        #print(c_score)\n",
    "        plt.figure()\n",
    "        plt.hist(c_score)\n",
    "        plt.show()\n",
    "\n",
    "#try_computing_c_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a1270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store contribution values for each sign distribution in files\n",
    "def get_and_store_c_scores(datasets, n_wts):\n",
    "    train_dataset, test_dataset = load_and_prep_dataset(\"CIFAR\", batch_size=2, shuffle_size=512)\n",
    "    model = CNN2Model_adapted()\n",
    "    model(list(train_dataset)[0][0])\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for i in range(n_wts):\n",
    "            train_dataset, test_dataset = load_and_prep_dataset(dataset, batch_size=1, shuffle_size=512)\n",
    "            model.load_weights(f\"1b WTs/h5_format/WT_{dataset}_IMP_{i}.h5\")\n",
    "            c_scores_1, c_scores_2 = contribution_scores(test_dataset, model)\n",
    "            np.save(f\"5d Contribution values/c_scores_dense1_{dataset}_wt{i}\", np.array(c_scores_1))\n",
    "            np.save(f\"5d Contribution values/c_scores_dense2_{dataset}_wt{i}\", np.array(c_scores_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906f9f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [48:29<00:00,  3.44it/s]\n"
     ]
    }
   ],
   "source": [
    "#get_and_store_c_scores([\"CIFAR\",\"CINIC\",\"SVHN\"],15)\n",
    "get_and_store_c_scores([\"CIFAR\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee2f0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot contribution values in scatterplot matrix as 5th dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f5e4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect statistics for contribution value of each cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
