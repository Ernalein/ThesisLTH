{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f26ae5bb",
   "metadata": {},
   "source": [
    "# Creating WTs with IMP\n",
    "\n",
    "This notebook contains code for:\n",
    "- importing necessary libraries\n",
    "- dowloading and preprocessing the dataset\n",
    "- using the dataset to train the full model and save the accuracies\n",
    "- performing IMP (with rewinding) using the model and datset\n",
    "- comparing pruned and unpruned model accuracies\n",
    "- saving sparse model weights of WTs in Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd8090f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries and the cnn architecture I defined\n",
    "\n",
    "from cnn_architecture import CNN2Model\n",
    "from utils import *\n",
    "from load_datasets import load_and_prep_dataset\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4178686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(16384, 256)\n",
      "(256,)\n",
      "(256, 256)\n",
      "(256,)\n",
      "(256, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# shape of the model weights\n",
    "\n",
    "train_dataset, test_dataset = load_and_prep_dataset(\"CIFAR\", batch_size=60, shuffle_size=512)\n",
    "model = CNN2Model()\n",
    "model(list(train_dataset)[0][0])\n",
    "\n",
    "weights = model.get_weights()\n",
    "biases = weights[1::2]\n",
    "not_biases = weights[0::2]\n",
    "\n",
    "for w in weights:\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5460b",
   "metadata": {},
   "source": [
    "### Train Loop and IMP loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c60f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified train loop to also work with sparse networks (such that pruned weights remain frozen at 0.0)\n",
    "\n",
    "def train_loop_sparse(train, test, model, num_epochs=5):\n",
    "    \n",
    "    # hyperparameters\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    loss_function= tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    # initializing training statistics\n",
    "    train_accuracy = tf.keras.metrics.Accuracy(name='test_accuracy')\n",
    "    test_accuracy = tf.keras.metrics.Accuracy(name='train_accuracy')\n",
    "    train_losses = tf.keras.metrics.CategoricalCrossentropy(name='train_losses')\n",
    "    test_losses = tf.keras.metrics.CategoricalCrossentropy(name='test_losses')\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    train_l =[]\n",
    "    test_l = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), leave=False, desc=\"training epochs\"):\n",
    "        \n",
    "        #train step\n",
    "        for x, t in train:\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = model(x)\n",
    "                loss = loss_function(t, pred)\n",
    "                train_losses.update_state(t, pred)\n",
    "                train_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            \n",
    "            # set gradients to zero for all pruned weights\n",
    "            new_gradients = []\n",
    "            for gradient_matrix, variables in zip(gradients, model.trainable_variables):\n",
    "                gradient_matrix = tf.where(variables == 0.0, 0.0, gradient_matrix)\n",
    "                new_gradients.append(gradient_matrix)\n",
    "            optimizer.apply_gradients(zip(new_gradients, model.trainable_variables))\n",
    "            \n",
    "        # test step\n",
    "        for x, t in test:\n",
    "            pred = model(x)\n",
    "            test_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "            test_losses.update_state(t, pred)\n",
    "        \n",
    "        # updataing training statistics\n",
    "        train_acc.append(train_accuracy.result().numpy())\n",
    "        test_acc.append(test_accuracy.result().numpy())\n",
    "        train_l.append(train_losses.result().numpy())\n",
    "        test_l.append(test_losses.result().numpy())\n",
    "        train_accuracy.reset_state()\n",
    "        test_accuracy.reset_state()\n",
    "        train_losses.reset_state()\n",
    "        test_losses.reset_state()     \n",
    "    \n",
    "    return  train_acc, test_acc, train_l, test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc01edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative magnitude pruning\n",
    "\n",
    "def imp(datasetname, train, test, model, iterations=7, epochs_per_it=5, pruning_rate_conv=15, pruning_rate_dense=30):\n",
    "    \n",
    "    # safe initial weights\n",
    "    initial_weights = model.get_weights()\n",
    "    \n",
    "    # initializing training statistics\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    train_l =[]\n",
    "    test_l = []\n",
    "    pruning_rates = [get_pruning_rates(initial_weights[::2])]   \n",
    "    \n",
    "    # iterations\n",
    "    for i in tqdm(range(iterations), position=0, desc=\"pruning iterations\"):\n",
    "        \n",
    "        \n",
    "        ##################################################\n",
    "        # Training #######################################\n",
    "        ##################################################\n",
    "\n",
    "        train_acc_i, test_acc_i, train_l_i, test_l_i = train_loop_sparse(train, test, model, num_epochs=epochs_per_it)\n",
    "        train_acc.extend(train_acc_i)\n",
    "        test_acc.extend(test_acc_i)\n",
    "        train_l.extend(train_l_i)\n",
    "        test_l.extend(test_l_i)              \n",
    "        \n",
    "        ##################################################\n",
    "        # Pruning ########################################  \n",
    "        ##################################################\n",
    "        \n",
    "\n",
    "        new_weights = copy.copy(initial_weights)\n",
    "        \n",
    "        # prune conv layers layer-wise:\n",
    "        new_conv_weights = []   \n",
    "        for conv_current, conv_init in zip(model.get_weights()[0:4:2], initial_weights[0:4:2]):\n",
    "            \n",
    "            # calculate percentile\n",
    "            conv_current_flat = conv_current.flatten()\n",
    "            conv_unpruned = copy.copy(conv_current_flat[conv_current_flat != 0.0])\n",
    "            conv_unpruned_abs =  tf.math.abs(conv_unpruned)\n",
    "            conv_percentile = np.percentile(conv_unpruned_abs, pruning_rate_conv)\n",
    "            \n",
    "            # prune according to percentile\n",
    "            conv_current_pruned = tf.where(tf.math.abs(conv_current) < conv_percentile, 0.0, conv_init)\n",
    "            new_conv_weights.append(conv_current_pruned)\n",
    "        \n",
    "        # replace the unpruned conv layers by the pruned conv layers\n",
    "        new_weights[0] = new_conv_weights[0] \n",
    "        new_weights[2] = new_conv_weights[1]      \n",
    "        \n",
    "        #---------\n",
    "        \n",
    "        # calculate percentile of dense layer globally\n",
    "        dense_unpruned_abs_all = []\n",
    "        for dense_layer in model.get_weights()[4::2]:\n",
    "            dense_flat = dense_layer.flatten()\n",
    "            dense_unpruned = copy.copy(dense_flat[dense_flat != 0.0])\n",
    "            dense_unpruned_abs =  tf.math.abs(dense_unpruned)\n",
    "            dense_unpruned_abs_all.extend(dense_unpruned_abs)\n",
    "        dense_percentile = np.percentile(dense_unpruned_abs_all, pruning_rate_dense)\n",
    "        \n",
    "        # prune dense layers according to percentile\n",
    "        new_dense_weights = []\n",
    "        for dense_current, dense_init in zip(model.get_weights()[4::2], initial_weights[4::2]):\n",
    "            \n",
    "            dense_pruned = tf.where(tf.math.abs(dense_current) < dense_percentile, 0.0, dense_init)\n",
    "            new_dense_weights.append(dense_pruned)\n",
    "        \n",
    "        # replace the unpruned dense layers by the pruned dense layers\n",
    "        new_weights[4] = new_dense_weights[0] \n",
    "        new_weights[6] = new_dense_weights[1]\n",
    "        new_weights[8] = new_dense_weights[2] \n",
    "\n",
    "        model.set_weights(new_weights)\n",
    "        pruning_rates.append(get_pruning_rates(new_weights[::2]))\n",
    "    \n",
    "    # plotting the pruning rates for each layer per iteration\n",
    "    pruning_rates = np.array(pruning_rates)\n",
    "    fig= plt.figure(figsize=(10,6))\n",
    "    plt.title(f\"Pruning rates IMP {datasetname} all layers\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"pruning_rates\")\n",
    "    plt.plot(pruning_rates[:,0],label=\"conv1\")\n",
    "    plt.plot(pruning_rates[:,1],label=\"conv2\")\n",
    "    plt.plot(pruning_rates[:,2],label=\"dense1\")\n",
    "    plt.plot(pruning_rates[:,3],label=\"dense2\")\n",
    "    plt.plot(pruning_rates[:,4],label=\"dense3\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"3b Plots/{datasetname}_IMP_pruning_rates.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # collecting losses in a dictionary\n",
    "    losses = { \"test loss\":test_l , \"training loss\":train_l , \"test accuracy\":test_acc , \"training accuracy\":train_acc}\n",
    "    \n",
    "    return  losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da114af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "def plot_losses(datasetname, train_acc, test_acc, train_l, test_l, title):\n",
    "    fig= plt.figure(figsize=(10,6))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.plot(test_l,label=\"test loss\")\n",
    "    plt.plot(train_l,label=\"training loss\")\n",
    "    plt.plot(test_acc,label=\"test accuracy\")\n",
    "    plt.plot(train_acc,label=\"training accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"3b Plots/{datasetname}_IMP_losses.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928067b",
   "metadata": {},
   "source": [
    "## Creating winning tickets from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92fb7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_net(datasetname, train_dataset, test_dataset):\n",
    "    #########################################\n",
    "    # 1.:  get initial weight configuration #\n",
    "    #########################################\n",
    "\n",
    "    model = CNN2Model()\n",
    "\n",
    "    #predict something with the model so it initializes the weights\n",
    "    model(list(train_dataset)[0][0])\n",
    "\n",
    "    #save the initial weights to reuse for IMP\n",
    "    initial_weights = model.get_weights()\n",
    "        \n",
    "    #########################################################################\n",
    "    # 2.:  train control model with initial weights to get control accuracy #\n",
    "    #########################################################################\n",
    "\n",
    "    control_model = CNN2Model()\n",
    "    control_model(list(train_dataset)[0][0])\n",
    "    control_model.set_weights(initial_weights)\n",
    "\n",
    "    control_losses = train_loop_sparse(train_dataset, test_dataset, control_model)\n",
    "    plot_losses(datasetname, \"Control\", control_losses,\"CNN Loss and Accuracy for control model\")\n",
    "    \n",
    "    #######################################################################################\n",
    "    # 3.:  use initial weights to also perform iterative magnitude pruning on a new model #\n",
    "    #######################################################################################\n",
    "\n",
    "    imp_model = CNN2Model()\n",
    "    imp_model(list(train_dataset)[0][0])\n",
    "    imp_model.set_weights(initial_weights)\n",
    "\n",
    "    imp_losses = imp(datasetname, train_dataset, test_dataset, imp_model)\n",
    "    plot_losses(datasetname, \"IMP\", imp_losses,\"CNN Loss and Accuracy during iterative magnitude pruning\")\n",
    "\n",
    "    #get resulting sparse network weights\n",
    "    sparse_weights = imp_model.get_weights()\n",
    "        \n",
    "    #############################################################################\n",
    "    # 4.:  train the resulting sparse network from the start and get accuracies #\n",
    "    #############################################################################\n",
    "\n",
    "    sparse_model = CNN2Model()\n",
    "    sparse_model(list(train_dataset)[0][0])\n",
    "    sparse_model.set_weights(sparse_weights)\n",
    "\n",
    "    sparse_losses = train_loop_sparse(train_dataset, test_dataset, sparse_model)\n",
    "    plot_losses(datasetname, \"IMP_WT\", sparse_losses,\"CNN Loss and Accuracy of sparse network\")\n",
    "    \n",
    "    return control_stats, sparse_stats, sparse_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f661abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lottery_tickets(runs, dataset):\n",
    "    \n",
    "\n",
    "    train_dataset, test_dataset = load_and_prep_dataset(dataset, batch_size=60, shuffle_size=512)    \n",
    "    \n",
    "    for r in range(runs):\n",
    "        \n",
    "        # create sparse network\n",
    "        control_stats, sparse_stats, sparse_weights = create_sparse_net(dataset, train_dataset, test_dataset)\n",
    "        control_accuracy = np.max(control_stats[1])\n",
    "        sparse_accuracy = np.max(sparse_stats[1])\n",
    "        print(\"Best accuracy of control model: \", control_accuracy)\n",
    "        print(\"Best accuracy of sparse model: \", sparse_accuracy)\n",
    "        \n",
    "        # calculate percentage of pruned weights\n",
    "        print(\"total pruning rate:\", get_pruning_rate(sparse_weights))\n",
    "        print(\"layerwise pruning rates:\", get_pruning_rates(sparse_weights))\n",
    "        \n",
    "        # compare accuracy to check wheter sparse network is winning ticket\n",
    "        if sparse_accuracy + 0.02 >= control_accuracy:\n",
    "            print(\"It's a winning ticket!\")\n",
    "            \n",
    "            # save WT model weights\n",
    "            sparse_model = CNN2Model()\n",
    "            sparse_model(list(train_dataset)[0][0])\n",
    "            sparse_model.set_weights(sparse_weights)\n",
    "            sparse_model.save_weights(f\"./1b WTs/WT_{dataset}_IMP_{r}\", save_format=\"h5\", overwrite=True)\n",
    "            \n",
    "        else:\n",
    "            print(\"It's not a winning ticket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_lottery_tickets(4, \"CINIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5986204b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
