{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fdad34",
   "metadata": {},
   "source": [
    "# Extracting sign distributions\n",
    "\n",
    "This notbook contains code for:\n",
    "- importing necessary libraries\n",
    "- loading stored WTs\n",
    "- creating random sparse networks\n",
    "- Extracting sign distributions from WTs\n",
    "- Extracting sign distributions from non-WTs\n",
    "- saving extracted distributions in files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784d557",
   "metadata": {},
   "source": [
    "### Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645bf7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 20:25:36.958789: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries and the cnn architecture I defined\n",
    "\n",
    "from cnn_architecture import CNN2Model\n",
    "from utils import *\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f037db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prep_cifar(batch_size, shuffle_size):\n",
    "    # load data set\n",
    "    (train_ds, test_ds), ds_info = tfds.load(name=\"cifar10\", split=[\"train\",\"test\"], as_supervised=True, with_info=True)\n",
    "    # tfds.show_examples(train_ds, ds_info)\n",
    "    \n",
    "    def prepare_cifar10_data(ds):\n",
    "        #convert data from uint8 to float32\n",
    "        ds = ds.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "        #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "        ds = ds.map(lambda img, target: ((img/128.)-1., target))\n",
    "        #create one-hot targets\n",
    "        ds = ds.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "        #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
    "        ds = ds.cache()\n",
    "        #shuffle, batch, prefetch\n",
    "        ds = ds.shuffle(shuffle_size).batch(batch_size).prefetch(2)\n",
    "        #return preprocessed dataset\n",
    "        return ds\n",
    "    \n",
    "    # prepare data\n",
    "    train_dataset = train_ds.apply(prepare_cifar10_data)\n",
    "    test_dataset = test_ds.apply(prepare_cifar10_data)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf5206ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sign distribution from one layer\n",
    "\n",
    "def sign_distribution_layer(this_layer, next_layer):\n",
    "    '''\n",
    "    columns: \n",
    "    [\"prune_rate_in\", \"prune_rate_out\", \"pos_in\", \"pos_out\", \"neg_in\", \"neg_out\", \"sign_rate_in\", \"sign_rate_out\"]\n",
    "    '''\n",
    "    sign_distr = pd.DataFrame()\n",
    "    unconnected_neurons = {\"no_incomming\": 0 , \"no_outgoing\" : 0, \"no_total_conncetions\" : 0}\n",
    "    for this_neuron, next_neurons, i in zip(this_layer.T, next_layer, range(np.shape(this_layer)[0])):\n",
    "        \n",
    "        this_unpruned = this_neuron[this_neuron != 0.0]\n",
    "        next_unpruned = next_neurons[next_neurons != 0.0]\n",
    "        \n",
    "        # only include neurons that receive and propagate input\n",
    "        if len(this_unpruned)*len(next_unpruned) != 0:\n",
    "            \n",
    "            dic = {} \n",
    "            dic[\"pos_in\"] = len(this_unpruned[this_unpruned>0])\n",
    "            dic[\"pos_out\"] = len(next_unpruned[next_unpruned>0])\n",
    "            dic[\"neg_in\"] = len(this_unpruned[this_unpruned<0])\n",
    "            dic[\"neg_out\"] = len(next_unpruned[next_unpruned<0])\n",
    "            dic[\"prune_rate_in\"] = 1 - (len(this_unpruned)/len(this_neuron))\n",
    "            dic[\"prune_rate_out\"] = 1 - (len(next_unpruned)/len(next_neurons))\n",
    "            dic[\"sign_rate_in\"] = dic[\"pos_in\"]/len(this_unpruned)   \n",
    "            dic[\"sign_rate_out\"] = dic[\"pos_out\"]/len(next_unpruned)                               \n",
    "\n",
    "            df = pd.DataFrame(data = dic, index = [i])\n",
    "            sign_distr = pd.concat([sign_distr, df], axis=0)\n",
    "        \n",
    "        # store information about inconnected neurons\n",
    "        else :\n",
    "            \n",
    "            if len(this_unpruned) == 0:\n",
    "                if len(next_unpruned) ==0:\n",
    "                    unconnected_neurons[\"no_total_conncetions\"] = unconnected_neurons[\"no_total_conncetions\"] + 1\n",
    "                else:\n",
    "                    unconnected_neurons[\"no_incomming\"] = unconnected_neurons[\"no_incomming\"] + 1\n",
    "            else:\n",
    "                unconnected_neurons[\"no_outgoing\"] = unconnected_neurons[\"no_outgoing\"] + 1\n",
    "        \n",
    "    return sign_distr, unconnected_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44795963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sign distribution form two layers\n",
    "\n",
    "def sign_distribution_layers(layer1, layer2, layer3):\n",
    "    \n",
    "    sign_distr1, unconnected_neurons1 = sign_distribution_layer(layer1, layer2)\n",
    "    sign_distr2, unconnected_neurons2 = sign_distribution_layer(layer2, layer3)\n",
    "    sign_distr1[\"layer\"] = \"dense1\"\n",
    "    sign_distr2[\"layer\"] = \"dense2\"\n",
    "    sign_distr_both = pd.concat([sign_distr1, sign_distr2], axis=0)\n",
    "    \n",
    "    unconnected_neurons1[\"layer\"] = \"dense1\"\n",
    "    unconnected_neurons2[\"layer\"] = \"dense2\"\n",
    "\n",
    "    return sign_distr_both, unconnected_neurons1, unconnected_neurons2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9bc0d",
   "metadata": {},
   "source": [
    "## Extracting sign distributions and storing them as files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8181b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_per_dataset = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc9ef673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sign distributions\n",
    "\n",
    "def get_sign_distr(dataset, pruning_name, n):\n",
    "\n",
    "    # collect statistics of unconnected neurons\n",
    "    unconnected_statistics = pd.DataFrame()\n",
    "    \n",
    "    # make a model to load the weights into\n",
    "    train_dataset, test_dataset = load_and_prep_cifar(batch_size=60, shuffle_size=512)\n",
    "    model = CNN2Model()\n",
    "    model(list(train_dataset)[0][0])\n",
    "    for i in tqdm(range(n), leave=False, desc=f\"sign_distributions of {dataset}\"):\n",
    "        \n",
    "        # get WT weights\n",
    "        model.load_weights(f\"1b WTs/WT_{dataset}_{pruning_name}_{i}\")    \n",
    "        weights_wt = model.get_weights()\n",
    "        # get sign distribution\n",
    "        sign_distr_wt, unconnected_neurons1, unconnected_neurons2 = sign_distribution_layers(weights_wt[4], weights_wt[6], weights_wt[8])\n",
    "        # store sign distribution\n",
    "        sign_distr_wt.to_csv(f'2b Sign distributions/{dataset}_{pruning_name}_{i}_sign_distr.csv', index=False)\n",
    "        \n",
    "        \n",
    "        #collect unconnected neurons statistics\n",
    "        unconnected_neurons1[\"model\"] = f\"WT_{dataset}_{pruning_name}_{i}\"\n",
    "        unconnected_neurons2[\"model\"] = f\"WT_{dataset}_{pruning_name}_{i}\"\n",
    "        df1 = pd.DataFrame(data = unconnected_neurons1, index = [i])\n",
    "        df2 = pd.DataFrame(data = unconnected_neurons2, index = [i])\n",
    "        unconnected_statistics = pd.concat([unconnected_statistics, df1, df2], axis=0)\n",
    "        \n",
    "    unconnected_statistics.to_csv(f'2b Sign distributions/{dataset}_{pruning_name}_unconnected_statistic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f57eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "# store sign distributions of Wts\n",
    "\n",
    "#for dataset in [\"SVHN\",\"CINIC\",\"CIFAR\"]:\n",
    "#    get_sign_distr(dataset,\"IMP\",wts_per_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84770de7",
   "metadata": {},
   "source": [
    "### Generating randomly pruned sign distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d79025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement random pruning of a weight matrix according to a pruning rate\n",
    "\n",
    "def random_pruning(model,pruning_rates):\n",
    "    \n",
    "    weights = model.get_weights()\n",
    "    new_weights = []\n",
    "    \n",
    "    for layer, p_rate in zip(weights[::2], pruning_rates):\n",
    "        shape = np.shape(layer)\n",
    "        layer = layer.flatten()\n",
    "        number_to_prune = int(len(layer) * p_rate)\n",
    "        pruning_indexi = random.sample(range(len(layer)),number_to_prune)\n",
    "        layer[pruning_indexi] = 0.0\n",
    "        layer = np.reshape(layer,shape)\n",
    "        new_weights.append(layer)\n",
    "        \n",
    "    weights[::2] = new_weights\n",
    "    model.set_weights(weights)\n",
    "    weights = model.get_weights()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e003e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function CNN2Model.call at 0x00000173CF82BEB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function CNN2Model.call at 0x00000173CF82BEB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function CNN2Model.call at 0x00000173CE394CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function CNN2Model.call at 0x00000173CE394CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "def make_random_sign_distr(n):\n",
    "\n",
    "    # get pruning rates from imp model\n",
    "    train_dataset, test_dataset = load_and_prep_cifar(batch_size=60, shuffle_size=512)\n",
    "    model_cifar_imp = CNN2Model()\n",
    "    model_cifar_imp(list(train_dataset)[0][0])\n",
    "    model_cifar_imp.load_weights(\"1b WTs/WT_CIFAR_IMP_0\")\n",
    "    pruning_rates = get_pruning_rates(model_cifar_imp.get_weights()[::2])\n",
    "\n",
    "    unconnected_statistics = pd.DataFrame()\n",
    "\n",
    "    # generate random sparse networks\n",
    "    for i in range(n):\n",
    "        \n",
    "        # randomly prune a model\n",
    "        random_model = CNN2Model()\n",
    "        random_model(list(train_dataset)[0][0])\n",
    "        random_model = random_pruning(random_model,pruning_rates)\n",
    "        weights_rsn = random_model.get_weights()\n",
    "        \n",
    "        # store sign distributions of random sparse networks\n",
    "        sign_distr_random, unconnected_neurons1, unconnected_neurons2  = sign_distribution_layers(weights_rsn[4], weights_rsn[6], weights_rsn[8])\n",
    "        sign_distr_random.to_csv(f\"2b Sign distributions/RSN_{i}_sign_distr.csv\", index=False)\n",
    "        \n",
    "        #collect unconnected neurons statistics\n",
    "        unconnected_neurons1[\"model\"] = f\"RSN_{i}\"\n",
    "        unconnected_neurons2[\"model\"] = f\"RSN_{i}\"\n",
    "        df1 = pd.DataFrame(data = unconnected_neurons1, index = [i])\n",
    "        df2 = pd.DataFrame(data = unconnected_neurons2, index = [i])\n",
    "        unconnected_statistics = pd.concat([unconnected_statistics, df1, df2], axis=0)\n",
    "\n",
    "    # store unconnected statistics\n",
    "    unconnected_statistics.to_csv(f'2b Sign distributions/RSN_unconnected_statistic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_random_sign_distr(wts_per_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144ff6a",
   "metadata": {},
   "source": [
    "## Get Pruning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97f64399",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_and_prep_cifar(batch_size=60, shuffle_size=512)\n",
    "model_cifar_imp = CNN2Model()\n",
    "model_cifar_imp(list(train_dataset)[0][0])\n",
    "model_cifar_imp.load_weights(\"1b WTs/WT_CIFAR_IMP_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b6b754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv rate (without biases):  0.6794931592039801\n",
      "dense rate (without biases):  0.9176456925675676\n",
      "conv rate (with biases):  0.6772469008264463\n",
      "dense rate (with biases):  0.9175333257329128\n",
      "pruning rates without biases each layer:  [0.6799768518518519, 0.6794704861111112, 0.9284100532531738, 0.259185791015625, 0.13789062500000004]\n",
      "pruning rate total with biases:  0.9153704562118372\n",
      "pruning rate total without biases:  0.9155087942502567\n"
     ]
    }
   ],
   "source": [
    "# return the pruning rates of the conv and dense layers\n",
    "print(\"conv rate (without biases): \", get_pruning_rate(model_cifar_imp.get_weights()[0:4:2]))\n",
    "print(\"dense rate (without biases): \", get_pruning_rate(model_cifar_imp.get_weights()[4::2]))\n",
    "print(\"conv rate (with biases): \", get_pruning_rate(model_cifar_imp.get_weights()[0:4:]))\n",
    "print(\"dense rate (with biases): \", get_pruning_rate(model_cifar_imp.get_weights()[4::]))\n",
    "print(\"pruning rates without biases each layer: \", get_pruning_rates(model_cifar_imp.get_weights()[::2]))\n",
    "print(\"pruning rate total with biases: \", get_pruning_rate(model_cifar_imp.get_weights()))\n",
    "print(\"pruning rate total without biases: \", get_pruning_rate(model_cifar_imp.get_weights()[::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67facc6",
   "metadata": {},
   "source": [
    "## Get sign distributions for different pruning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9551ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sign_distr(dataset, p_rate, n):\n",
    "\n",
    "    # collect statistics of unconnected neurons\n",
    "    unconnected_statistics = pd.DataFrame()\n",
    "    \n",
    "    # make a model to load the weights into\n",
    "    train_dataset, test_dataset = load_and_prep_cifar(batch_size=60, shuffle_size=512)\n",
    "    model = CNN2Model()\n",
    "    model(list(train_dataset)[0][0])\n",
    "    for i in tqdm(range(n), leave=False, desc=f\"sign_distributions of {dataset}\"):\n",
    "        \n",
    "        # get WT weights\n",
    "        model.load_weights(f\"1b WTs/more_p_rates/WT_{dataset}_IMP{p_rate}_{i}\")    \n",
    "        weights_wt = model.get_weights()\n",
    "        # get sign distribution\n",
    "        sign_distr_wt, unconnected_neurons1, unconnected_neurons2 = sign_distribution_layers(weights_wt[4], weights_wt[6], weights_wt[8])\n",
    "        # store sign distribution\n",
    "        sign_distr_wt.to_csv(f'2b Sign distributions/{dataset}_IMP{p_rate}_{i}_sign_distr.csv', index=False)\n",
    "        \n",
    "        \n",
    "        #collect unconnected neurons statistics\n",
    "        unconnected_neurons1[\"model\"] = f\"WT_{dataset}_IMP{p_rate}_{i}\"\n",
    "        unconnected_neurons2[\"model\"] = f\"WT_{dataset}_IMP{p_rate}_{i}\"\n",
    "        df1 = pd.DataFrame(data = unconnected_neurons1, index = [i])\n",
    "        df2 = pd.DataFrame(data = unconnected_neurons2, index = [i])\n",
    "        unconnected_statistics = pd.concat([unconnected_statistics, df1, df2], axis=0)\n",
    "        \n",
    "    unconnected_statistics.to_csv(f'2b Sign distributions/more_p_rates/{dataset}_IMP{p_rate}_unconnected_statistic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1356bce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/student/e/epetersen/tensorflow_datasets/cifar10/3.0.2...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 20:27:15.547310: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20258eb98f654047a14f1b7020599316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c232b035ca754beabf5c0dbe3e43286b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac208bc6274b43c387875682b416e5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d747ee91449748f4923c2ef0655793cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9843a198d55140ba8137303106baff5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 20:27:30.812205: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 20:27:30.885020: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 20:27:30.885328: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 20:27:30.886197: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 20:27:30.886459: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 20:27:30.886665: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 20:27:30.941078: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 20:27:30.941312: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 20:27:30.941481: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 20:27:30.941578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2895 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de261a5616747ed91de3f9a634b373a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/student/e/epetersen/tensorflow_datasets/cifar10/3.0.2.incompleteILZH7A/cifar10-train.tfrecord*…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb134c4340e4741a4863a9c94bade0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d6c71842814821a97973b986b966ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/student/e/epetersen/tensorflow_datasets/cifar10/3.0.2.incompleteILZH7A/cifar10-test.tfrecord*.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset cifar10 downloaded and prepared to /home/student/e/epetersen/tensorflow_datasets/cifar10/3.0.2. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/layers/pooling/base_pooling.py:23: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(name=name, **kwargs)\n",
      "2024-05-29 20:28:03.997811: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-05-29 20:28:04.237577: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "                                                                   \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File format not supported: filepath=1b WTs/more_p_rates/WT_CIFAR_IMP30_0. Keras 3 only supports V3 `.keras` and `.weights.h5` files, or legacy V1/V2 `.h5` files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m p_rates \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m51\u001b[39m,\u001b[38;5;241m65\u001b[39m,\u001b[38;5;241m76\u001b[39m,\u001b[38;5;241m80\u001b[39m,\u001b[38;5;241m83\u001b[39m,\u001b[38;5;241m88\u001b[39m,\u001b[38;5;241m92\u001b[39m,\u001b[38;5;241m94\u001b[39m,\u001b[38;5;241m96\u001b[39m,\u001b[38;5;241m97\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p_rate \u001b[38;5;129;01min\u001b[39;00m p_rates:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mget_sign_distr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCIFAR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mp_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mget_sign_distr\u001b[0;34m(dataset, p_rate, n)\u001b[0m\n\u001b[1;32m      9\u001b[0m model(\u001b[38;5;28mlist\u001b[39m(train_dataset)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msign_distributions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# get WT weights\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1b WTs/more_p_rates/WT_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_IMP\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mp_rate\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     14\u001b[0m     weights_wt \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_weights()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# get sign distribution\u001b[39;00m\n",
      "File \u001b[0;32m/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_weights\u001b[0;34m(model, filepath, skip_mismatch, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             legacy_h5_format\u001b[38;5;241m.\u001b[39mload_weights_from_hdf5_group(f, model)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` and `.weights.h5` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles, or legacy V1/V2 `.h5` files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: File format not supported: filepath=1b WTs/more_p_rates/WT_CIFAR_IMP30_0. Keras 3 only supports V3 `.keras` and `.weights.h5` files, or legacy V1/V2 `.h5` files."
     ]
    }
   ],
   "source": [
    "p_rates = [30,51,65,76,80,83,88,92,94,96,97]\n",
    "for p_rate in p_rates:\n",
    "    get_sign_distr(\"CIFAR\",p_rate,n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
