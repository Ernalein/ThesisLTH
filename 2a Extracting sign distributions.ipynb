{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fdad34",
   "metadata": {},
   "source": [
    "# Extracting sign distributions\n",
    "\n",
    "This notbook contains code for:\n",
    "- importing necessary libraries\n",
    "- loading stored WTs\n",
    "- creating random sparse networks\n",
    "- Extracting sign distributions from WTs\n",
    "- Extracting sign distributions from non-WTs\n",
    "- saving extracted distributions in files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784d557",
   "metadata": {},
   "source": [
    "### Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645bf7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries and the cnn architecture I defined\n",
    "\n",
    "from cnn_architecture import CNN2Model\n",
    "from utils import *\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f037db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prep_cifar(batch_size, shuffle_size):\n",
    "    # load data set\n",
    "    (train_ds, test_ds), ds_info = tfds.load(name=\"cifar10\", split=[\"train\",\"test\"], as_supervised=True, with_info=True)\n",
    "    # tfds.show_examples(train_ds, ds_info)\n",
    "    \n",
    "    def prepare_cifar10_data(ds):\n",
    "        #convert data from uint8 to float32\n",
    "        ds = ds.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "        #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "        ds = ds.map(lambda img, target: ((img/128.)-1., target))\n",
    "        #create one-hot targets\n",
    "        ds = ds.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "        #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
    "        ds = ds.cache()\n",
    "        #shuffle, batch, prefetch\n",
    "        ds = ds.shuffle(shuffle_size).batch(batch_size).prefetch(2)\n",
    "        #return preprocessed dataset\n",
    "        return ds\n",
    "    \n",
    "    # prepare data\n",
    "    train_dataset = train_ds.apply(prepare_cifar10_data)\n",
    "    test_dataset = test_ds.apply(prepare_cifar10_data)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5206ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sign distribution from one layer\n",
    "\n",
    "def sign_distribution_layer(this_layer, next_layer):\n",
    "    '''\n",
    "    columns: \n",
    "    [\"prune_rate_in\", \"prune_rate_out\", \"pos_in\", \"pos_out\", \"neg_in\", \"neg_out\", \"sign_rate_in\", \"sign_rate_out\"]\n",
    "    '''\n",
    "    sign_distr = pd.DataFrame()\n",
    "    unconnected_neurons = {\"no_incomming\": 0 , \"no_outgoing\" : 0, \"no_total_conncetions\" : 0}\n",
    "    for this_neuron, next_neurons, i in zip(this_layer.T, next_layer, range(np.shape(this_layer)[0])):\n",
    "        \n",
    "        this_unpruned = this_neuron[this_neuron != 0.0]\n",
    "        next_unpruned = next_neurons[next_neurons != 0.0]\n",
    "        \n",
    "        # only include neurons that receive and propagate input\n",
    "        if len(this_unpruned)*len(next_unpruned) != 0:\n",
    "            \n",
    "            dic = {} \n",
    "            dic[\"pos_in\"] = len(this_unpruned[this_unpruned>0])\n",
    "            dic[\"pos_out\"] = len(next_unpruned[next_unpruned>0])\n",
    "            dic[\"neg_in\"] = len(this_unpruned[this_unpruned<0])\n",
    "            dic[\"neg_out\"] = len(next_unpruned[next_unpruned<0])\n",
    "            dic[\"prune_rate_in\"] = 1 - (len(this_unpruned)/len(this_neuron))\n",
    "            dic[\"prune_rate_out\"] = 1 - (len(next_unpruned)/len(next_neurons))\n",
    "            dic[\"sign_rate_in\"] = dic[\"pos_in\"]/len(this_unpruned)   \n",
    "            dic[\"sign_rate_out\"] = dic[\"pos_out\"]/len(next_unpruned)                               \n",
    "\n",
    "            df = pd.DataFrame(data = dic, index = [i])\n",
    "            sign_distr = pd.concat([sign_distr, df], axis=0)\n",
    "        \n",
    "        # store information about inconnected neurons\n",
    "        else :\n",
    "            \n",
    "            if len(this_unpruned) == 0:\n",
    "                if len(next_unpruned) ==0:\n",
    "                    unconnected_neurons[\"no_total_conncetions\"] = unconnected_neurons[\"no_total_conncetions\"] + 1\n",
    "                else:\n",
    "                    unconnected_neurons[\"no_incomming\"] = unconnected_neurons[\"no_incomming\"] + 1\n",
    "            else:\n",
    "                unconnected_neurons[\"no_outgoing\"] = unconnected_neurons[\"no_outgoing\"] + 1\n",
    "        \n",
    "    return sign_distr, unconnected_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44795963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sign distribution form two layers\n",
    "\n",
    "def sign_distribution_layers(layer1, layer2, layer3):\n",
    "    \n",
    "    sign_distr1, unconnected_neurons1 = sign_distribution_layer(layer1, layer2)\n",
    "    sign_distr2, unconnected_neurons2 = sign_distribution_layer(layer2, layer3)\n",
    "    sign_distr1[\"layer\"] = \"dense1\"\n",
    "    sign_distr2[\"layer\"] = \"dense2\"\n",
    "    sign_distr_both = pd.concat([sign_distr1, sign_distr2], axis=0)\n",
    "    \n",
    "    unconnected_neurons1[\"layer\"] = \"dense1\"\n",
    "    unconnected_neurons2[\"layer\"] = \"dense2\"\n",
    "\n",
    "    return sign_distr_both, unconnected_neurons1, unconnected_neurons2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9bc0d",
   "metadata": {},
   "source": [
    "## Extracting sign distributions and storing them as files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8181b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_per_dataset = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc9ef673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sign distributions\n",
    "\n",
    "def get_sign_distr(dataset, pruning_name, n):\n",
    "\n",
    "    # collect statistics of unconnected neurons\n",
    "    unconnected_statistics = pd.DataFrame()\n",
    "    \n",
    "    # make a model to load the weights into\n",
    "    train_dataset, test_dataset = load_and_prep_cifar(batch_size=60, shuffle_size=512)\n",
    "    model = CNN2Model()\n",
    "    model(list(train_dataset)[0][0])\n",
    "    for i in tqdm(range(n), leave=False, desc=f\"sign_distributions of {dataset}\"):\n",
    "        \n",
    "        # get WT weights\n",
    "        model.load_weights(f\"1b WTs/WT_{dataset}_{pruning_name}_{i}\")    \n",
    "        weights_wt = model.get_weights()\n",
    "        # get sign distribution\n",
    "        sign_distr_wt, unconnected_neurons1, unconnected_neurons2 = sign_distribution_layers(weights_wt[4], weights_wt[6], weights_wt[8])\n",
    "        # store sign distribution\n",
    "        sign_distr_wt.to_csv(f'2b Sign distributions/{dataset}_{pruning_name}_{i}_sign_distr.csv', index=False)\n",
    "        \n",
    "        \n",
    "        #collect unconnected neurons statistics\n",
    "        unconnected_neurons1[\"model\"] = f\"WT_{dataset}_{pruning_name}_{i}\"\n",
    "        unconnected_neurons2[\"model\"] = f\"WT_{dataset}_{pruning_name}_{i}\"\n",
    "        df1 = pd.DataFrame(data = unconnected_neurons1, index = [i])\n",
    "        df2 = pd.DataFrame(data = unconnected_neurons2, index = [i])\n",
    "        unconnected_statistics = pd.concat([unconnected_statistics, df1, df2], axis=0)\n",
    "        \n",
    "    unconnected_statistics.to_csv(f'2b Sign distributions/{dataset}_{pruning_name}_unconnected_statistic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f57eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "# store sign distributions of Wts\n",
    "\n",
    "for dataset in [\"SVHN\",\"CINIC\",\"CIFAR\"]:\n",
    "    get_sign_distr(dataset,\"IMP\",wts_per_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84770de7",
   "metadata": {},
   "source": [
    "### Generating randomly pruned sign distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d79025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement random pruning of a weight matrix according to a pruning rate\n",
    "\n",
    "def random_pruning(model,pruning_rates):\n",
    "    \n",
    "    weights = model.get_weights()\n",
    "    new_weights = []\n",
    "    \n",
    "    for layer, p_rate in zip(weights[::2], pruning_rates):\n",
    "        shape = np.shape(layer)\n",
    "        layer = layer.flatten()\n",
    "        number_to_prune = int(len(layer) * p_rate)\n",
    "        pruning_indexi = random.sample(range(len(layer)),number_to_prune)\n",
    "        layer[pruning_indexi] = 0.0\n",
    "        layer = np.reshape(layer,shape)\n",
    "        new_weights.append(layer)\n",
    "        \n",
    "    weights[::2] = new_weights\n",
    "    model.set_weights(weights)\n",
    "    weights = model.get_weights()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e003e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function CNN2Model.call at 0x00000173CF82BEB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function CNN2Model.call at 0x00000173CF82BEB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function CNN2Model.call at 0x00000173CE394CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function CNN2Model.call at 0x00000173CE394CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# get pruning rates from imp model\n",
    "train_dataset, test_dataset = load_and_prep_cifar(batch_size=60, shuffle_size=512)\n",
    "model_cifar_imp = CNN2Model()\n",
    "model_cifar_imp(list(train_dataset)[0][0])\n",
    "model_cifar_imp.load_weights(\"1b WTs/WT_CIFAR_IMP_0\")\n",
    "pruning_rates = get_pruning_rates(model_cifar_imp.get_weights()[::2])\n",
    "\n",
    "unconnected_statistics = pd.DataFrame()\n",
    "\n",
    "# generate random sparse networks\n",
    "for i in range(wts_per_dataset):\n",
    "    \n",
    "    # randomly prune a model\n",
    "    random_model = CNN2Model()\n",
    "    random_model(list(train_dataset)[0][0])\n",
    "    random_model = random_pruning(random_model,pruning_rates)\n",
    "    weights_rsn = random_model.get_weights()\n",
    "    \n",
    "    # store sign distributions of random sparse networks\n",
    "    sign_distr_random, unconnected_neurons1, unconnected_neurons2  = sign_distribution_layers(weights_rsn[4], weights_rsn[6], weights_rsn[8])\n",
    "    sign_distr_random.to_csv(f\"2b Sign distributions/RSN_{i}_sign_distr.csv\", index=False)\n",
    "    \n",
    "    #collect unconnected neurons statistics\n",
    "    unconnected_neurons1[\"model\"] = f\"RSN_{i}\"\n",
    "    unconnected_neurons2[\"model\"] = f\"RSN_{i}\"\n",
    "    df1 = pd.DataFrame(data = unconnected_neurons1, index = [i])\n",
    "    df2 = pd.DataFrame(data = unconnected_neurons2, index = [i])\n",
    "    unconnected_statistics = pd.concat([unconnected_statistics, df1, df2], axis=0)\n",
    "\n",
    "# store unconnected statistics\n",
    "unconnected_statistics.to_csv(f'2b Sign distributions/RSN_unconnected_statistic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144ff6a",
   "metadata": {},
   "source": [
    "## Pruning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97f64399",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_and_prep_cifar(batch_size=60, shuffle_size=512)\n",
    "model_cifar_imp = CNN2Model()\n",
    "model_cifar_imp(list(train_dataset)[0][0])\n",
    "model_cifar_imp.load_weights(\"1b WTs/WT_CIFAR_IMP_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b6b754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv rate (without biases):  0.6794931592039801\n",
      "dense rate (without biases):  0.9176456925675676\n",
      "conv rate (with biases):  0.6772469008264463\n",
      "dense rate (with biases):  0.9175333257329128\n",
      "pruning rates without biases each layer:  [0.6799768518518519, 0.6794704861111112, 0.9284100532531738, 0.259185791015625, 0.13789062500000004]\n",
      "pruning rate total with biases:  0.9153704562118372\n",
      "pruning rate total without biases:  0.9155087942502567\n"
     ]
    }
   ],
   "source": [
    "# return the pruning rates of the conv and dense layers\n",
    "print(\"conv rate (without biases): \", get_pruning_rate(model_cifar_imp.get_weights()[0:4:2]))\n",
    "print(\"dense rate (without biases): \", get_pruning_rate(model_cifar_imp.get_weights()[4::2]))\n",
    "print(\"conv rate (with biases): \", get_pruning_rate(model_cifar_imp.get_weights()[0:4:]))\n",
    "print(\"dense rate (with biases): \", get_pruning_rate(model_cifar_imp.get_weights()[4::]))\n",
    "print(\"pruning rates without biases each layer: \", get_pruning_rates(model_cifar_imp.get_weights()[::2]))\n",
    "print(\"pruning rate total with biases: \", get_pruning_rate(model_cifar_imp.get_weights()))\n",
    "print(\"pruning rate total without biases: \", get_pruning_rate(model_cifar_imp.get_weights()[::2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1356bce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
