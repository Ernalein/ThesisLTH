{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fdad34",
   "metadata": {},
   "source": [
    "# Extracting sign distributions\n",
    "\n",
    "This notbook contains code for:\n",
    "- importing necessary libraries\n",
    "- loading stored WTs\n",
    "- creating random sparse networks\n",
    "- Extracting sign distributions from WTs\n",
    "- Extracting sign distributions from non-WTs\n",
    "- saving extracted distributions in files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784d557",
   "metadata": {},
   "source": [
    "### Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645bf7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 21:56:02.892158: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis_no_gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "from cnn_architecture import CNN2Model\n",
    "from utils import *\n",
    "from load_datasets import load_and_prep_dataset\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5206ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sign distribution from one layer\n",
    "\n",
    "def sign_distribution_layer(this_layer, next_layer):\n",
    "    '''\n",
    "    columns: \n",
    "    [\"prune_rate_in\", \"prune_rate_out\", \"pos_in\", \"pos_out\", \"neg_in\", \"neg_out\", \"sign_rate_in\", \"sign_rate_out\"]\n",
    "    '''\n",
    "    sign_distr = pd.DataFrame()\n",
    "    unconnected_neurons = {\"no_incomming\": 0 , \"no_outgoing\" : 0, \"no_total_conncetions\" : 0}\n",
    "    for this_neuron, next_neurons, i in zip(this_layer.T, next_layer, range(np.shape(this_layer)[0])):\n",
    "        \n",
    "        this_unpruned = this_neuron[this_neuron != 0.0]\n",
    "        next_unpruned = next_neurons[next_neurons != 0.0]\n",
    "        \n",
    "        # only include neurons that receive and propagate input\n",
    "        if len(this_unpruned)*len(next_unpruned) != 0:\n",
    "            \n",
    "            dic = {} \n",
    "            dic[\"pos_in\"] = len(this_unpruned[this_unpruned>0])\n",
    "            dic[\"pos_out\"] = len(next_unpruned[next_unpruned>0])\n",
    "            dic[\"neg_in\"] = len(this_unpruned[this_unpruned<0])\n",
    "            dic[\"neg_out\"] = len(next_unpruned[next_unpruned<0])\n",
    "            dic[\"prune_rate_in\"] = 1 - (len(this_unpruned)/len(this_neuron))\n",
    "            dic[\"prune_rate_out\"] = 1 - (len(next_unpruned)/len(next_neurons))\n",
    "            dic[\"sign_rate_in\"] = dic[\"pos_in\"]/len(this_unpruned)   \n",
    "            dic[\"sign_rate_out\"] = dic[\"pos_out\"]/len(next_unpruned)                               \n",
    "\n",
    "            df = pd.DataFrame(data = dic, index = [i])\n",
    "            sign_distr = pd.concat([sign_distr, df], axis=0)\n",
    "        \n",
    "        # store information about inconnected neurons\n",
    "        else :\n",
    "            \n",
    "            if len(this_unpruned) == 0:\n",
    "                if len(next_unpruned) ==0:\n",
    "                    unconnected_neurons[\"no_total_conncetions\"] = unconnected_neurons[\"no_total_conncetions\"] + 1\n",
    "                else:\n",
    "                    unconnected_neurons[\"no_incomming\"] = unconnected_neurons[\"no_incomming\"] + 1\n",
    "            else:\n",
    "                unconnected_neurons[\"no_outgoing\"] = unconnected_neurons[\"no_outgoing\"] + 1\n",
    "        \n",
    "    return sign_distr, unconnected_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44795963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sign distribution form two layers\n",
    "\n",
    "def sign_distribution_layers(layer1, layer2, layer3):\n",
    "    \n",
    "    sign_distr1, unconnected_neurons1 = sign_distribution_layer(layer1, layer2)\n",
    "    sign_distr2, unconnected_neurons2 = sign_distribution_layer(layer2, layer3)\n",
    "    sign_distr1[\"layer\"] = \"dense1\"\n",
    "    sign_distr2[\"layer\"] = \"dense2\"\n",
    "    sign_distr_both = pd.concat([sign_distr1, sign_distr2], axis=0)\n",
    "    \n",
    "    unconnected_neurons1[\"layer\"] = \"dense1\"\n",
    "    unconnected_neurons2[\"layer\"] = \"dense2\"\n",
    "\n",
    "    return sign_distr_both, unconnected_neurons1, unconnected_neurons2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9bc0d",
   "metadata": {},
   "source": [
    "## Extracting sign distributions and storing them as files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8181b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_per_dataset = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc9ef673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sign distributions\n",
    "\n",
    "def get_sign_distr(dataset, pruning_name, n):\n",
    "\n",
    "    # collect statistics of unconnected neurons\n",
    "    unconnected_statistics = pd.DataFrame()\n",
    "    \n",
    "    # make a model to load the weights into\n",
    "    train_dataset, test_dataset = load_and_prep_dataset(dataset, batch_size=60, shuffle_size=512)\n",
    "    model = CNN2Model()\n",
    "    model(list(train_dataset)[0][0])\n",
    "    for i in tqdm(range(n), leave=False, desc=f\"sign_distributions of {dataset}\"):\n",
    "        \n",
    "        # get WT weights\n",
    "        model.load_weights(f\"1b WTs/WT_{dataset}_{pruning_name}_{i}.h5\")    \n",
    "        weights_wt = model.get_weights()\n",
    "        # get sign distribution\n",
    "        sign_distr_wt, unconnected_neurons1, unconnected_neurons2 = sign_distribution_layers(weights_wt[4], weights_wt[6], weights_wt[8])\n",
    "        # store sign distribution\n",
    "        sign_distr_wt.to_csv(f'2b Sign distributions/{dataset}_{pruning_name}_{i}_sign_distr.csv', index=False)\n",
    "        \n",
    "        \n",
    "        #collect unconnected neurons statistics\n",
    "        unconnected_neurons1[\"model\"] = f\"WT_{dataset}_{pruning_name}_{i}\"\n",
    "        unconnected_neurons2[\"model\"] = f\"WT_{dataset}_{pruning_name}_{i}\"\n",
    "        df1 = pd.DataFrame(data = unconnected_neurons1, index = [i])\n",
    "        df2 = pd.DataFrame(data = unconnected_neurons2, index = [i])\n",
    "        unconnected_statistics = pd.concat([unconnected_statistics, df1, df2], axis=0)\n",
    "        \n",
    "    unconnected_statistics.to_csv(f'2b Sign distributions/{dataset}_{pruning_name}_unconnected_statistic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f57eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store sign distributions of Wts\n",
    "\n",
    "#for dataset in [\"SVHN\",\"CINIC\",\"CIFAR\"]:\n",
    "#    get_sign_distr(dataset,\"IMP\",wts_per_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84770de7",
   "metadata": {},
   "source": [
    "### Generating randomly pruned sign distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d79025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement random pruning of a weight matrix according to a pruning rate\n",
    "\n",
    "def random_pruning(model,pruning_rates):\n",
    "    \n",
    "    weights = model.get_weights()\n",
    "    new_weights = []\n",
    "    \n",
    "    for layer, p_rate in zip(weights[::2], pruning_rates):\n",
    "        shape = np.shape(layer)\n",
    "        layer = layer.flatten()\n",
    "        number_to_prune = int(len(layer) * p_rate)\n",
    "        pruning_indexi = random.sample(range(len(layer)),number_to_prune)\n",
    "        layer[pruning_indexi] = 0.0\n",
    "        layer = np.reshape(layer,shape)\n",
    "        new_weights.append(layer)\n",
    "        \n",
    "    weights[::2] = new_weights\n",
    "    model.set_weights(weights)\n",
    "    weights = model.get_weights()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e003e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_sign_distr(n):\n",
    "\n",
    "    # get pruning rates from imp model\n",
    "    train_dataset, test_dataset = load_and_prep_dataset(\"CIFAR\",batch_size=60, shuffle_size=512)\n",
    "    model_cifar_imp = CNN2Model()\n",
    "    model_cifar_imp(list(train_dataset)[0][0])\n",
    "    model_cifar_imp.load_weights(\"1b WTs/h5_format/WT_CIFAR_IMP_0.h5\")\n",
    "    pruning_rates = get_pruning_rates(model_cifar_imp.get_weights()[::2])\n",
    "\n",
    "    unconnected_statistics = pd.DataFrame()\n",
    "\n",
    "    # generate random sparse networks\n",
    "    for i in range(n):\n",
    "        \n",
    "        # randomly prune a model\n",
    "        random_model = CNN2Model()\n",
    "        random_model(list(train_dataset)[0][0])\n",
    "        random_model = random_pruning(random_model,pruning_rates)\n",
    "        random_model.save_weights(f\"./1b WTs/h5_format/RSN_{i}.h5\", overwrite=True)\n",
    "        weights_rsn = random_model.get_weights()\n",
    "        \n",
    "        # store sign distributions of random sparse networks\n",
    "        sign_distr_random, unconnected_neurons1, unconnected_neurons2  = sign_distribution_layers(weights_rsn[4], weights_rsn[6], weights_rsn[8])\n",
    "        sign_distr_random.to_csv(f\"2b Sign distributions/RSN_{i}_sign_distr.csv\", index=False)\n",
    "        \n",
    "        #collect unconnected neurons statistics\n",
    "        unconnected_neurons1[\"model\"] = f\"RSN_{i}\"\n",
    "        unconnected_neurons2[\"model\"] = f\"RSN_{i}\"\n",
    "        df1 = pd.DataFrame(data = unconnected_neurons1, index = [i])\n",
    "        df2 = pd.DataFrame(data = unconnected_neurons2, index = [i])\n",
    "        unconnected_statistics = pd.concat([unconnected_statistics, df1, df2], axis=0)\n",
    "\n",
    "    # store unconnected statistics\n",
    "    unconnected_statistics.to_csv(f'2b Sign distributions/RSN_unconnected_statistic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d8f9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_random_sign_distr(wts_per_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144ff6a",
   "metadata": {},
   "source": [
    "## Get Pruning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97f64399",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_and_prep_dataset(\"CIFAR\", batch_size=60, shuffle_size=512)\n",
    "model_cifar_imp = CNN2Model()\n",
    "model_cifar_imp(list(train_dataset)[0][0])\n",
    "model_cifar_imp.load_weights(\"1b WTs/h5_format/WT_CIFAR_IMP_0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00b6b754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv rate (without biases):  0.6794931592039801\n",
      "dense rate (without biases):  0.9176456925675676\n",
      "conv rate (with biases):  0.6772469008264463\n",
      "dense rate (with biases):  0.9175333257329128\n",
      "pruning rates without biases each layer:  [0.6799768518518519, 0.6794704861111112, 0.9284100532531738, 0.259185791015625, 0.13789062500000004]\n",
      "pruning rates with biases each layer: \n",
      "     0.6556919642857143\n",
      "     0.6782928942807626\n",
      "     0.9283533910588954\n",
      "     0.2581772859922179\n",
      "     0.1373540856031128\n",
      "pruning rate total with biases:  0.9153704562118372\n",
      "pruning rate total without biases:  0.9155087942502567\n"
     ]
    }
   ],
   "source": [
    "# return the pruning rates of the conv and dense layers\n",
    "print(\"conv rate (without biases): \", get_pruning_rate(model_cifar_imp.get_weights()[0:4:2]))\n",
    "print(\"dense rate (without biases): \", get_pruning_rate(model_cifar_imp.get_weights()[4::2]))\n",
    "print(\"conv rate (with biases): \", get_pruning_rate(model_cifar_imp.get_weights()[0:4:]))\n",
    "print(\"dense rate (with biases): \", get_pruning_rate(model_cifar_imp.get_weights()[4::]))\n",
    "print(\"pruning rates without biases each layer: \", get_pruning_rates(model_cifar_imp.get_weights()[::2]))\n",
    "print(\"pruning rates with biases each layer: \")\n",
    "print(\"    \", get_pruning_rate(model_cifar_imp.get_weights()[0:2]))\n",
    "print(\"    \", get_pruning_rate(model_cifar_imp.get_weights()[2:4]))\n",
    "print(\"    \", get_pruning_rate(model_cifar_imp.get_weights()[4:6]))\n",
    "print(\"    \", get_pruning_rate(model_cifar_imp.get_weights()[6:8]))\n",
    "print(\"    \", get_pruning_rate(model_cifar_imp.get_weights()[8:10]))\n",
    "\n",
    "print(\"pruning rate total with biases: \", get_pruning_rate(model_cifar_imp.get_weights()))\n",
    "print(\"pruning rate total without biases: \", get_pruning_rate(model_cifar_imp.get_weights()[::2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72721226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpruned parameter counts layer types\n",
      "conv rate (without biases):  12369\n",
      "dense rate (without biases):  351027\n",
      "conv rate (with biases):  12497\n",
      "dense rate (with biases):  351549\n",
      "\n",
      "unpruned parameter counts individual layers\n",
      "without biases each layer:  [553, 11816, 300270, 48550, 2207]\n",
      "with biases each layer: \n",
      "     617\n",
      "     11880\n",
      "     300526\n",
      "     48806\n",
      "     2217\n",
      "\n",
      "unpruned parameter counts total with biases:  364046\n",
      "unpruned parameter counts total without biases:  363396\n"
     ]
    }
   ],
   "source": [
    "# return the parameter counts of the conv and dense layers\n",
    "print(\"unpruned parameter counts layer types\")\n",
    "print(\"conv rate (without biases): \", get_unpruned_params_count(model_cifar_imp.get_weights()[0:4:2]))\n",
    "print(\"dense rate (without biases): \", get_unpruned_params_count(model_cifar_imp.get_weights()[4::2]))\n",
    "print(\"conv rate (with biases): \", get_unpruned_params_count(model_cifar_imp.get_weights()[0:4:]))\n",
    "print(\"dense rate (with biases): \", get_unpruned_params_count(model_cifar_imp.get_weights()[4::]))\n",
    "print(\"\")\n",
    "print(\"unpruned parameter counts individual layers\")\n",
    "print(\"without biases each layer: \", get_unpruned_params_counts(model_cifar_imp.get_weights()[::2]))\n",
    "print(\"with biases each layer: \")\n",
    "print(\"    \", get_unpruned_params_count(model_cifar_imp.get_weights()[0:2]))\n",
    "print(\"    \", get_unpruned_params_count(model_cifar_imp.get_weights()[2:4]))\n",
    "print(\"    \", get_unpruned_params_count(model_cifar_imp.get_weights()[4:6]))\n",
    "print(\"    \", get_unpruned_params_count(model_cifar_imp.get_weights()[6:8]))\n",
    "print(\"    \", get_unpruned_params_count(model_cifar_imp.get_weights()[8:10]))\n",
    "print(\"\")\n",
    "print(\"unpruned parameter counts total with biases: \", get_unpruned_params_count(model_cifar_imp.get_weights()))\n",
    "print(\"unpruned parameter counts total without biases: \", get_unpruned_params_count(model_cifar_imp.get_weights()[::2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2d016d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter counts layer types\n",
      "conv rate (without biases):  38592\n",
      "dense rate (without biases):  4262400\n",
      "conv rate (with biases):  38720\n",
      "dense rate (with biases):  4262922\n",
      "\n",
      "parameter counts individual layers\n",
      "without biases each layer:  [553, 11816, 300270, 48550, 2207]\n",
      "with biases each layer: \n",
      "     1792\n",
      "     36928\n",
      "     4194560\n",
      "     65792\n",
      "     2570\n",
      "\n",
      "unpruned parameter counts total with biases:  4301642\n",
      "unpruned parameter counts total without biases:  4300992\n"
     ]
    }
   ],
   "source": [
    "# return the parameter counts of the conv and dense layers\n",
    "print(\"parameter counts layer types\")\n",
    "print(\"conv rate (without biases): \", get_params_count(model_cifar_imp.get_weights()[0:4:2]))\n",
    "print(\"dense rate (without biases): \", get_params_count(model_cifar_imp.get_weights()[4::2]))\n",
    "print(\"conv rate (with biases): \", get_params_count(model_cifar_imp.get_weights()[0:4:]))\n",
    "print(\"dense rate (with biases): \", get_params_count(model_cifar_imp.get_weights()[4::]))\n",
    "print(\"\")\n",
    "print(\"parameter counts individual layers\")\n",
    "print(\"without biases each layer: \", get_unpruned_params_counts(model_cifar_imp.get_weights()[::2]))\n",
    "print(\"with biases each layer: \")\n",
    "print(\"    \", get_params_count(model_cifar_imp.get_weights()[0:2]))\n",
    "print(\"    \", get_params_count(model_cifar_imp.get_weights()[2:4]))\n",
    "print(\"    \", get_params_count(model_cifar_imp.get_weights()[4:6]))\n",
    "print(\"    \", get_params_count(model_cifar_imp.get_weights()[6:8]))\n",
    "print(\"    \", get_params_count(model_cifar_imp.get_weights()[8:10]))\n",
    "print(\"\")\n",
    "print(\"unpruned parameter counts total with biases: \", get_params_count(model_cifar_imp.get_weights()))\n",
    "print(\"unpruned parameter counts total without biases: \", get_params_count(model_cifar_imp.get_weights()[::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67facc6",
   "metadata": {},
   "source": [
    "## Get sign distributions for different pruning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9551ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sign_distr(dataset, p_rate, n):\n",
    "\n",
    "    # collect statistics of unconnected neurons\n",
    "    unconnected_statistics = pd.DataFrame()\n",
    "    \n",
    "    # make a model to load the weights into\n",
    "    train_dataset, test_dataset = load_and_prep_dataset(\"CIFAR\", batch_size=60, shuffle_size=512)\n",
    "    model = CNN2Model()\n",
    "    model(list(train_dataset)[0][0])\n",
    "    for i in tqdm(range(n), leave=False, desc=f\"sign_distributions with p rate {p_rate}\"):\n",
    "        \n",
    "        # get WT weights\n",
    "        model.load_weights(f\"1b WTs/more_p_rates/h5_format/WT_{dataset}_IMP{p_rate}_{i}.h5\")    \n",
    "        weights_wt = model.get_weights()\n",
    "        # get sign distribution\n",
    "        sign_distr_wt, unconnected_neurons1, unconnected_neurons2 = sign_distribution_layers(weights_wt[4], weights_wt[6], weights_wt[8])\n",
    "        # store sign distribution\n",
    "        sign_distr_wt.to_csv(f'2b Sign distributions/more_p_rates/{dataset}_IMP{p_rate}_{i}_sign_distr.csv', index=False)\n",
    "        \n",
    "        \n",
    "        #collect unconnected neurons statistics\n",
    "        unconnected_neurons1[\"model\"] = f\"WT_{dataset}_IMP{p_rate}_{i}\"\n",
    "        unconnected_neurons2[\"model\"] = f\"WT_{dataset}_IMP{p_rate}_{i}\"\n",
    "        df1 = pd.DataFrame(data = unconnected_neurons1, index = [i])\n",
    "        df2 = pd.DataFrame(data = unconnected_neurons2, index = [i])\n",
    "        unconnected_statistics = pd.concat([unconnected_statistics, df1, df2], axis=0)\n",
    "        \n",
    "    unconnected_statistics.to_csv(f'2b Sign distributions/more_p_rates/{dataset}_IMP{p_rate}_unconnected_statistic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07c99651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1504629629629629, 0.0, 0.15001085069444442, 0.0, 0.303910493850708, 0.0, 0.0600128173828125, 0.0, 0.03671875000000002, 0.0]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_and_prep_dataset(\"CIFAR\", batch_size=60, shuffle_size=512)\n",
    "model = CNN2Model()\n",
    "model(list(train_dataset)[0][0])\n",
    "model.load_weights(f\"1b WTs/more_p_rates/h5_format/WT_CIFAR_IMP30_0.h5\")\n",
    "print(get_pruning_rates(model.get_weights()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1356bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_rates = [30,51,65,76,80,83,88,92,94,96,97]\n",
    "p_rates = [30,51,65,76,83,88,92,94,96,97]\n",
    "#for p_rate in p_rates:\n",
    "    #get_sign_distr(\"CIFAR\",p_rate,n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80569d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
