{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fdad34",
   "metadata": {},
   "source": [
    "# Extracting sign distributions\n",
    "\n",
    "This notbook contains code for:\n",
    "- importing necessary libraries\n",
    "- loading stored WTs\n",
    "- creating random sparse networks\n",
    "- Extracting sign distributions from WTs\n",
    "- Extracting sign distributions from non-WTs\n",
    "- saving extracted distributions in files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784d557",
   "metadata": {},
   "source": [
    "### Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645bf7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 16:38:04.064045: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries and the cnn architecture I defined\n",
    "\n",
    "from cnn_architecture import CNN2Model\n",
    "from utils import *\n",
    "from load_datasets import load_and_prep_dataset\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf5206ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sign distribution from one layer\n",
    "\n",
    "def sign_distribution_layer(this_layer, next_layer):\n",
    "    '''\n",
    "    columns: \n",
    "    [\"prune_rate_in\", \"prune_rate_out\", \"pos_in\", \"pos_out\", \"neg_in\", \"neg_out\", \"sign_rate_in\", \"sign_rate_out\"]\n",
    "    '''\n",
    "    sign_distr = pd.DataFrame()\n",
    "    unconnected_neurons = {\"no_incomming\": 0 , \"no_outgoing\" : 0, \"no_total_conncetions\" : 0}\n",
    "    for this_neuron, next_neurons, i in zip(this_layer.T, next_layer, range(np.shape(this_layer)[0])):\n",
    "        \n",
    "        this_unpruned = this_neuron[this_neuron != 0.0]\n",
    "        next_unpruned = next_neurons[next_neurons != 0.0]\n",
    "        \n",
    "        # only include neurons that receive and propagate input\n",
    "        if len(this_unpruned)*len(next_unpruned) != 0:\n",
    "            \n",
    "            dic = {} \n",
    "            dic[\"pos_in\"] = len(this_unpruned[this_unpruned>0])\n",
    "            dic[\"pos_out\"] = len(next_unpruned[next_unpruned>0])\n",
    "            dic[\"neg_in\"] = len(this_unpruned[this_unpruned<0])\n",
    "            dic[\"neg_out\"] = len(next_unpruned[next_unpruned<0])\n",
    "            dic[\"prune_rate_in\"] = 1 - (len(this_unpruned)/len(this_neuron))\n",
    "            dic[\"prune_rate_out\"] = 1 - (len(next_unpruned)/len(next_neurons))\n",
    "            dic[\"sign_rate_in\"] = dic[\"pos_in\"]/len(this_unpruned)   \n",
    "            dic[\"sign_rate_out\"] = dic[\"pos_out\"]/len(next_unpruned)                               \n",
    "\n",
    "            df = pd.DataFrame(data = dic, index = [i])\n",
    "            sign_distr = pd.concat([sign_distr, df], axis=0)\n",
    "        \n",
    "        # store information about inconnected neurons\n",
    "        else :\n",
    "            \n",
    "            if len(this_unpruned) == 0:\n",
    "                if len(next_unpruned) ==0:\n",
    "                    unconnected_neurons[\"no_total_conncetions\"] = unconnected_neurons[\"no_total_conncetions\"] + 1\n",
    "                else:\n",
    "                    unconnected_neurons[\"no_incomming\"] = unconnected_neurons[\"no_incomming\"] + 1\n",
    "            else:\n",
    "                unconnected_neurons[\"no_outgoing\"] = unconnected_neurons[\"no_outgoing\"] + 1\n",
    "        \n",
    "    return sign_distr, unconnected_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44795963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sign distribution form two layers\n",
    "\n",
    "def sign_distribution_layers(layer1, layer2, layer3):\n",
    "    \n",
    "    sign_distr1, unconnected_neurons1 = sign_distribution_layer(layer1, layer2)\n",
    "    sign_distr2, unconnected_neurons2 = sign_distribution_layer(layer2, layer3)\n",
    "    sign_distr1[\"layer\"] = \"dense1\"\n",
    "    sign_distr2[\"layer\"] = \"dense2\"\n",
    "    sign_distr_both = pd.concat([sign_distr1, sign_distr2], axis=0)\n",
    "    \n",
    "    unconnected_neurons1[\"layer\"] = \"dense1\"\n",
    "    unconnected_neurons2[\"layer\"] = \"dense2\"\n",
    "\n",
    "    return sign_distr_both, unconnected_neurons1, unconnected_neurons2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9bc0d",
   "metadata": {},
   "source": [
    "## Extracting sign distributions and storing them as files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8181b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_per_dataset = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc9ef673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sign distributions\n",
    "\n",
    "def get_sign_distr(dataset, pruning_name, n):\n",
    "\n",
    "    # collect statistics of unconnected neurons\n",
    "    unconnected_statistics = pd.DataFrame()\n",
    "    \n",
    "    # make a model to load the weights into\n",
    "    train_dataset, test_dataset = load_and_prep_dataset(dataset, batch_size=60, shuffle_size=512)\n",
    "    model = CNN2Model()\n",
    "    model(list(train_dataset)[0][0])\n",
    "    for i in tqdm(range(n), leave=False, desc=f\"sign_distributions of {dataset}\"):\n",
    "        \n",
    "        # get WT weights\n",
    "        model.load_weights(f\"1b WTs/WT_{dataset}_{pruning_name}_{i}\")    \n",
    "        weights_wt = model.get_weights()\n",
    "        # get sign distribution\n",
    "        sign_distr_wt, unconnected_neurons1, unconnected_neurons2 = sign_distribution_layers(weights_wt[4], weights_wt[6], weights_wt[8])\n",
    "        # store sign distribution\n",
    "        sign_distr_wt.to_csv(f'2b Sign distributions/{dataset}_{pruning_name}_{i}_sign_distr.csv', index=False)\n",
    "        \n",
    "        \n",
    "        #collect unconnected neurons statistics\n",
    "        unconnected_neurons1[\"model\"] = f\"WT_{dataset}_{pruning_name}_{i}\"\n",
    "        unconnected_neurons2[\"model\"] = f\"WT_{dataset}_{pruning_name}_{i}\"\n",
    "        df1 = pd.DataFrame(data = unconnected_neurons1, index = [i])\n",
    "        df2 = pd.DataFrame(data = unconnected_neurons2, index = [i])\n",
    "        unconnected_statistics = pd.concat([unconnected_statistics, df1, df2], axis=0)\n",
    "        \n",
    "    unconnected_statistics.to_csv(f'2b Sign distributions/{dataset}_{pruning_name}_unconnected_statistic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f57eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "# store sign distributions of Wts\n",
    "\n",
    "#for dataset in [\"SVHN\",\"CINIC\",\"CIFAR\"]:\n",
    "#    get_sign_distr(dataset,\"IMP\",wts_per_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84770de7",
   "metadata": {},
   "source": [
    "### Generating randomly pruned sign distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d79025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement random pruning of a weight matrix according to a pruning rate\n",
    "\n",
    "def random_pruning(model,pruning_rates):\n",
    "    \n",
    "    weights = model.get_weights()\n",
    "    new_weights = []\n",
    "    \n",
    "    for layer, p_rate in zip(weights[::2], pruning_rates):\n",
    "        shape = np.shape(layer)\n",
    "        layer = layer.flatten()\n",
    "        number_to_prune = int(len(layer) * p_rate)\n",
    "        pruning_indexi = random.sample(range(len(layer)),number_to_prune)\n",
    "        layer[pruning_indexi] = 0.0\n",
    "        layer = np.reshape(layer,shape)\n",
    "        new_weights.append(layer)\n",
    "        \n",
    "    weights[::2] = new_weights\n",
    "    model.set_weights(weights)\n",
    "    weights = model.get_weights()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e003e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function CNN2Model.call at 0x00000173CF82BEB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function CNN2Model.call at 0x00000173CF82BEB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function CNN2Model.call at 0x00000173CE394CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function CNN2Model.call at 0x00000173CE394CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "def make_random_sign_distr(n):\n",
    "\n",
    "    # get pruning rates from imp model\n",
    "    train_dataset, test_dataset = load_and_prep_cifar(batch_size=60, shuffle_size=512)\n",
    "    model_cifar_imp = CNN2Model()\n",
    "    model_cifar_imp(list(train_dataset)[0][0])\n",
    "    model_cifar_imp.load_weights(\"1b WTs/WT_CIFAR_IMP_0\")\n",
    "    pruning_rates = get_pruning_rates(model_cifar_imp.get_weights()[::2])\n",
    "\n",
    "    unconnected_statistics = pd.DataFrame()\n",
    "\n",
    "    # generate random sparse networks\n",
    "    for i in range(n):\n",
    "        \n",
    "        # randomly prune a model\n",
    "        random_model = CNN2Model()\n",
    "        random_model(list(train_dataset)[0][0])\n",
    "        random_model = random_pruning(random_model,pruning_rates)\n",
    "        weights_rsn = random_model.get_weights()\n",
    "        \n",
    "        # store sign distributions of random sparse networks\n",
    "        sign_distr_random, unconnected_neurons1, unconnected_neurons2  = sign_distribution_layers(weights_rsn[4], weights_rsn[6], weights_rsn[8])\n",
    "        sign_distr_random.to_csv(f\"2b Sign distributions/RSN_{i}_sign_distr.csv\", index=False)\n",
    "        \n",
    "        #collect unconnected neurons statistics\n",
    "        unconnected_neurons1[\"model\"] = f\"RSN_{i}\"\n",
    "        unconnected_neurons2[\"model\"] = f\"RSN_{i}\"\n",
    "        df1 = pd.DataFrame(data = unconnected_neurons1, index = [i])\n",
    "        df2 = pd.DataFrame(data = unconnected_neurons2, index = [i])\n",
    "        unconnected_statistics = pd.concat([unconnected_statistics, df1, df2], axis=0)\n",
    "\n",
    "    # store unconnected statistics\n",
    "    unconnected_statistics.to_csv(f'2b Sign distributions/RSN_unconnected_statistic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8f9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_random_sign_distr(wts_per_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144ff6a",
   "metadata": {},
   "source": [
    "## Get Pruning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97f64399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 22:18:42.192152: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 22:18:42.246078: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 22:18:42.246369: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 22:18:42.248249: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 22:18:42.248595: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 22:18:42.248804: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 22:18:42.314017: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 22:18:42.314301: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 22:18:42.314478: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-29 22:18:42.314579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 190 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/layers/pooling/base_pooling.py:23: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(name=name, **kwargs)\n",
      "2024-05-29 22:18:45.249772: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-05-29 22:18:45.354417: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "2024-05-29 22:18:45.594315: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 96.75MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-05-29 22:18:45.606639: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 88.56MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-05-29 22:18:45.693553: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 151.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-05-29 22:18:45.701296: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 151.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-05-29 22:18:45.701337: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 271.89MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File format not supported: filepath=1b WTs/WT_CIFAR_IMP_0. Keras 3 only supports V3 `.keras` and `.weights.h5` files, or legacy V1/V2 `.h5` files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model_cifar_imp \u001b[38;5;241m=\u001b[39m CNN2Model()\n\u001b[1;32m      3\u001b[0m model_cifar_imp(\u001b[38;5;28mlist\u001b[39m(train_dataset)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel_cifar_imp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1b WTs/WT_CIFAR_IMP_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_weights\u001b[0;34m(model, filepath, skip_mismatch, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             legacy_h5_format\u001b[38;5;241m.\u001b[39mload_weights_from_hdf5_group(f, model)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` and `.weights.h5` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles, or legacy V1/V2 `.h5` files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: File format not supported: filepath=1b WTs/WT_CIFAR_IMP_0. Keras 3 only supports V3 `.keras` and `.weights.h5` files, or legacy V1/V2 `.h5` files."
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_and_prep_dataset(\"CIFAR\", batch_size=60, shuffle_size=512)\n",
    "model_cifar_imp = CNN2Model()\n",
    "model_cifar_imp(list(train_dataset)[0][0])\n",
    "model_cifar_imp.load_weights(\"1b WTs/WT_CIFAR_IMP_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b6b754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv rate (without biases):  0.6794931592039801\n",
      "dense rate (without biases):  0.9176456925675676\n",
      "conv rate (with biases):  0.6772469008264463\n",
      "dense rate (with biases):  0.9175333257329128\n",
      "pruning rates without biases each layer:  [0.6799768518518519, 0.6794704861111112, 0.9284100532531738, 0.259185791015625, 0.13789062500000004]\n",
      "pruning rate total with biases:  0.9153704562118372\n",
      "pruning rate total without biases:  0.9155087942502567\n"
     ]
    }
   ],
   "source": [
    "# return the pruning rates of the conv and dense layers\n",
    "print(\"conv rate (without biases): \", get_pruning_rate(model_cifar_imp.get_weights()[0:4:2]))\n",
    "print(\"dense rate (without biases): \", get_pruning_rate(model_cifar_imp.get_weights()[4::2]))\n",
    "print(\"conv rate (with biases): \", get_pruning_rate(model_cifar_imp.get_weights()[0:4:]))\n",
    "print(\"dense rate (with biases): \", get_pruning_rate(model_cifar_imp.get_weights()[4::]))\n",
    "print(\"pruning rates without biases each layer: \", get_pruning_rates(model_cifar_imp.get_weights()[::2]))\n",
    "print(\"pruning rates with biases each layer: \")\n",
    "print(get_pruning_rate(model_cifar_imp.get_weights()[[0,1]]))\n",
    "print(get_pruning_rate(model_cifar_imp.get_weights()[[2,3]]))\n",
    "print(get_pruning_rate(model_cifar_imp.get_weights()[[4,5]]))\n",
    "print(get_pruning_rate(model_cifar_imp.get_weights()[[6,7]]))\n",
    "print(get_pruning_rate(model_cifar_imp.get_weights()[[8,9]]))\n",
    "\n",
    "print(\"pruning rate total with biases: \", get_pruning_rate(model_cifar_imp.get_weights()))\n",
    "print(\"pruning rate total without biases: \", get_pruning_rate(model_cifar_imp.get_weights()[::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67facc6",
   "metadata": {},
   "source": [
    "## Get sign distributions for different pruning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9551ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sign_distr(dataset, p_rate, n):\n",
    "\n",
    "    # collect statistics of unconnected neurons\n",
    "    unconnected_statistics = pd.DataFrame()\n",
    "    \n",
    "    # make a model to load the weights into\n",
    "    train_dataset, test_dataset = load_and_prep_dataset(\"CIFAR\", batch_size=60, shuffle_size=512)\n",
    "    model = CNN2Model()\n",
    "    model(list(train_dataset)[0][0])\n",
    "    for i in tqdm(range(n), leave=False, desc=f\"sign_distributions of {dataset}\"):\n",
    "        \n",
    "        # get WT weights\n",
    "        model.load_weights(f\"1b WTs/more_p_rates/WT_{dataset}_IMP{p_rate}_{i}.weights.h5\")    \n",
    "        weights_wt = model.get_weights()\n",
    "        # get sign distribution\n",
    "        sign_distr_wt, unconnected_neurons1, unconnected_neurons2 = sign_distribution_layers(weights_wt[4], weights_wt[6], weights_wt[8])\n",
    "        # store sign distribution\n",
    "        sign_distr_wt.to_csv(f'2b Sign distributions/{dataset}_IMP{p_rate}_{i}_sign_distr.csv', index=False)\n",
    "        \n",
    "        \n",
    "        #collect unconnected neurons statistics\n",
    "        unconnected_neurons1[\"model\"] = f\"WT_{dataset}_IMP{p_rate}_{i}\"\n",
    "        unconnected_neurons2[\"model\"] = f\"WT_{dataset}_IMP{p_rate}_{i}\"\n",
    "        df1 = pd.DataFrame(data = unconnected_neurons1, index = [i])\n",
    "        df2 = pd.DataFrame(data = unconnected_neurons2, index = [i])\n",
    "        unconnected_statistics = pd.concat([unconnected_statistics, df1, df2], axis=0)\n",
    "        \n",
    "    unconnected_statistics.to_csv(f'2b Sign distributions/more_p_rates/{dataset}_IMP{p_rate}_unconnected_statistic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c99651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis/lib/python3.11/site-packages/keras/src/layers/pooling/base_pooling.py:23: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(name=name, **kwargs)\n",
      "2024-05-30 16:39:25.809411: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1504629629629629, 0.0, 0.15001085069444442, 0.0, 0.3039360046386719, 0.0, 0.058380126953125, 0.0, 0.037109375, 0.0]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_and_prep_dataset(\"CIFAR\", batch_size=60, shuffle_size=512)\n",
    "model = CNN2Model()\n",
    "model(list(train_dataset)[0][0])\n",
    "model.load_weights(f\"1b WTs/more_p_rates/WT_CIFAR_IMP30_0.weights.h5\")\n",
    "print(get_pruning_rates(model.get_weights()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1356bce0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_sign_distr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m p_rates \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m51\u001b[39m,\u001b[38;5;241m65\u001b[39m,\u001b[38;5;241m76\u001b[39m,\u001b[38;5;241m83\u001b[39m,\u001b[38;5;241m88\u001b[39m,\u001b[38;5;241m92\u001b[39m,\u001b[38;5;241m94\u001b[39m,\u001b[38;5;241m96\u001b[39m,\u001b[38;5;241m97\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p_rate \u001b[38;5;129;01min\u001b[39;00m p_rates:\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mget_sign_distr\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIFAR\u001b[39m\u001b[38;5;124m\"\u001b[39m,p_rate,n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_sign_distr' is not defined"
     ]
    }
   ],
   "source": [
    "p_rates = [30,51,65,76,80,83,88,92,94,96,97]\n",
    "for p_rate in p_rates:\n",
    "    get_sign_distr(\"CIFAR\",p_rate,n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80569d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
