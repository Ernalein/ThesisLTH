{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3d387b",
   "metadata": {},
   "source": [
    "# Optimizing Masks to create WTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fed2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries and the cnn architecture I defined\n",
    "\n",
    "from cnn_architecture import CNN2Model\n",
    "from utils import *\n",
    "from load_datasets import load_and_prep_dataset\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "import copy\n",
    "\n",
    "# all the extra stuff for supermasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279914f",
   "metadata": {},
   "source": [
    "all the variables i have to check their meaning:\n",
    "- use bias\n",
    "- dynamik scaling\n",
    "- sigmoid bias\n",
    "- use learning phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a9fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedDense(tf.keras.layers.Dense):\n",
    "    \n",
    "    # untrainable normal Dense layer\n",
    "    # trainable mask, that is sigmoided (maybe squished) and then multiplied to Dense\n",
    "    \n",
    "    def __init__(self, units,*args, **kwargs):\n",
    "        super(MaskedDense, self).__init__(units, *args, **kwargs)        \n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(MaskedDense, self).build(input_shape)\n",
    "        mask_init = tf.random.uniform(shape=self.kernel.shape,minval=-1, maxval=1, seed=None)\n",
    "\n",
    "        self._trainable_weights.remove(self.kernel)\n",
    "        self._non_trainable_weights.append(self.kernel)\n",
    "        self._trainable_weights.remove(self.bias)\n",
    "        self._non_trainable_weights.append(self.bias)\n",
    "        \n",
    "        self.kernel_mask = tf.Variable(initial_value=mask_init,\n",
    "                                        trainable=True,\n",
    "                                        validate_shape=True,\n",
    "                                        caching_device=None,\n",
    "                                        name='mask',\n",
    "                                        dtype=self.dtype,\n",
    "                                        shape=self.kernel.shape)\n",
    "\n",
    "        #self.kernel_mask = tf.get_variable('mask',\n",
    "        #                                   shape=self.kernel.shape,\n",
    "        #                                   dtype=self.dtype,\n",
    "        #                                   initializer=mask_init,\n",
    "        #                                   trainable=True)\n",
    "        self._trainable_weights.append(self.kernel_mask)\n",
    "    \n",
    "    \n",
    "    def get_effective_mask(self):\n",
    "        # during train, clamp all of them to 0's and 1's sampled by bernoulli and backprop the probabilities\n",
    "        # during test, clamp all of them to their rounded values\n",
    "        # actually, sample them too\n",
    "        which_to_clamp = tf.ones(self.kernel_mask.shape)\n",
    "        binary_mask = lambda: tf.cast(tfp.distributions.Bernoulli(probs=tf.nn.sigmoid(self.kernel_mask)).sample(), dtype=tf.float32)\n",
    "\n",
    "        return which_to_clamp * binary_mask + (1 - which_to_clamp) * tf.nn.sigmoid(self.kernel_mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # same as original call() except round some sample to {0, 1} based on a sample\n",
    "    def call(self, inputs):\n",
    "\n",
    "        effective_mask = tf.cast(tfp.distributions.Bernoulli(probs=tf.nn.sigmoid(self.kernel_mask)).sample(), dtype=tf.float32)\n",
    "        effective_kernel = tf.math.multiply(self.kernel, effective_mask)\n",
    "\n",
    "        #if self.dynamic_scaling:\n",
    "            #self.ones_in_mask = tf.reduce_sum(effective_mask)\n",
    "            #self.multiplier = tf.div(tf.to_float(tf.size(effective_mask)), self.ones_in_mask)\n",
    "            #effective_kernel = self.multiplier * effective_kernel\n",
    "\n",
    "        # original code from https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/keras/layers/core.py:\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "        #print(\"inputs: \", inputs.shape)\n",
    "        #print(\"kernel: \", effective_kernel.shape)\n",
    "        outputs = tf.linalg.matmul(inputs, effective_kernel)\n",
    "        outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def get_mask(self):\n",
    "        return  tf.nn.sigmoid(self.kernel_mask)\n",
    "    \n",
    "    def get_binary_mask(self):\n",
    "        return tf.math.round(tf.nn.sigmoid(self.kernel_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517a819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2ModelMasked(tf.keras.Model):\n",
    "    \n",
    "    # basic\n",
    "    def __init__(self):\n",
    "        super(CNN2ModelMasked, self).__init__()\n",
    "        \n",
    "        # set biases to a value that is not exactly 0.0, so they don't get handled like pruned values\n",
    "        self.bias_in = tf.keras.initializers.Constant(value=0.0000000001)\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=3,activation=\"relu\", padding=\"same\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batchsize,32,32,64]\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3,activation=\"relu\", padding=\"same\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batchsize,32,32,64]\n",
    "        self.maxpool = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2, 2),input_shape=(32, 32, 64)) # [batchsize,16,16,64]\n",
    "        self.flatten = tf.keras.layers.Flatten() # [batch_size,16384]\n",
    "        self.dense1 = MaskedDense(256, activation=\"relu\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batch_size,256]\n",
    "        self.dense2 = MaskedDense(256, activation=\"relu\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batch_size,256]\n",
    "        self.dense3 = MaskedDense(10, activation=\"softmax\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batch_size,256]\n",
    "        \n",
    "        # Making the weights of the conv layers untrainable\n",
    "        self.conv1.trainable = False\n",
    "        self.conv2.trainable = False\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # adjust the dense layers to be multiplayed with trainable mask (which gets assigned binary values for this step)\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    def get_masks(self):\n",
    "        return [self.dense1.get_mask(), self.dense2.get_mask(), self.dense3.get_mask()]\n",
    "        \n",
    "    def get_binary_masks(self):\n",
    "        return [self.dense1.get_binary_mask(), self.dense2.get_binary_mask(), self.dense3.get_binary_mask()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42718cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified train loop to also work with sparse networks (such that pruned weights remain frozen at 0.0)\n",
    "\n",
    "def train_mask(train, test, model, num_epochs=5):\n",
    "    \n",
    "    # hyperparameters\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    loss_function= tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    # initializing training statistics\n",
    "    train_accuracy = tf.keras.metrics.Accuracy(name='test_accuracy')\n",
    "    test_accuracy = tf.keras.metrics.Accuracy(name='train_accuracy')\n",
    "    train_losses = tf.keras.metrics.CategoricalCrossentropy(name='train_losses')\n",
    "    test_losses = tf.keras.metrics.CategoricalCrossentropy(name='test_losses')\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    train_l =[]\n",
    "    test_l = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), leave=False, desc=\"training epochs\"):\n",
    "        \n",
    "        #train step\n",
    "        for x, t in train:\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = model(x)\n",
    "                loss = loss_function(t, pred)\n",
    "                train_losses.update_state(t, pred)\n",
    "                train_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "        # test step\n",
    "        for x, t in test:\n",
    "            pred = model(x)\n",
    "            test_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "            test_losses.update_state(t, pred)\n",
    "        \n",
    "        # updataing training statistics\n",
    "        train_acc.append(train_accuracy.result().numpy())\n",
    "        test_acc.append(test_accuracy.result().numpy())\n",
    "        train_l.append(train_losses.result().numpy())\n",
    "        test_l.append(test_losses.result().numpy())\n",
    "        train_accuracy.reset_state()\n",
    "        test_accuracy.reset_state()\n",
    "        train_losses.reset_state()\n",
    "        test_losses.reset_state()\n",
    "        \n",
    "    # collecting losses in a dictionary\n",
    "    losses = { \"test loss\":test_l , \"training loss\":train_l , \"test accuracy\":test_acc , \"training accuracy\":train_acc}\n",
    "    \n",
    "    return  losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cc4a3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(16384, 256), dtype=float32, numpy=\n",
      "array([[0.4581738 , 0.5799989 , 0.4621882 , ..., 0.52411056, 0.36852047,\n",
      "        0.67673594],\n",
      "       [0.43519253, 0.6635228 , 0.58269286, ..., 0.40841594, 0.4972405 ,\n",
      "        0.6766733 ],\n",
      "       [0.3719052 , 0.4826121 , 0.626531  , ..., 0.706634  , 0.68747675,\n",
      "        0.5611489 ],\n",
      "       ...,\n",
      "       [0.65025574, 0.4183221 , 0.43523479, ..., 0.38998443, 0.60151505,\n",
      "        0.4569376 ],\n",
      "       [0.6262519 , 0.44637433, 0.27587998, ..., 0.30124587, 0.5658544 ,\n",
      "        0.49375638],\n",
      "       [0.57453865, 0.5532706 , 0.4838324 , ..., 0.3154695 , 0.64649326,\n",
      "        0.515374  ]], dtype=float32)>, <tf.Tensor: shape=(256, 256), dtype=float32, numpy=\n",
      "array([[0.72930586, 0.62081826, 0.5630467 , ..., 0.5992753 , 0.7032884 ,\n",
      "        0.42649275],\n",
      "       [0.6091988 , 0.68663853, 0.546972  , ..., 0.33514825, 0.66069543,\n",
      "        0.3636892 ],\n",
      "       [0.71843505, 0.39507845, 0.47479647, ..., 0.67338884, 0.30302197,\n",
      "        0.6124765 ],\n",
      "       ...,\n",
      "       [0.71929264, 0.5871257 , 0.45760947, ..., 0.44113305, 0.5433455 ,\n",
      "        0.38616538],\n",
      "       [0.2764429 , 0.6407995 , 0.6620931 , ..., 0.33666673, 0.29377282,\n",
      "        0.58701164],\n",
      "       [0.38147056, 0.6600807 , 0.65128046, ..., 0.68283343, 0.58250624,\n",
      "        0.35266876]], dtype=float32)>, <tf.Tensor: shape=(256, 10), dtype=float32, numpy=\n",
      "array([[0.6637919 , 0.42173162, 0.34878826, ..., 0.6143286 , 0.32688823,\n",
      "        0.66682255],\n",
      "       [0.63299745, 0.42746434, 0.52746534, ..., 0.36051846, 0.47815147,\n",
      "        0.50540984],\n",
      "       [0.6092406 , 0.30570084, 0.3528146 , ..., 0.60360295, 0.50388795,\n",
      "        0.5411755 ],\n",
      "       ...,\n",
      "       [0.416238  , 0.3759527 , 0.6770604 , ..., 0.57810533, 0.52391356,\n",
      "        0.3703197 ],\n",
      "       [0.4872949 , 0.7094705 , 0.4584571 , ..., 0.44661927, 0.27019194,\n",
      "        0.42547265],\n",
      "       [0.7096395 , 0.58562946, 0.6256874 , ..., 0.3511082 , 0.5243352 ,\n",
      "        0.49019092]], dtype=float32)>]\n",
      "[<tf.Tensor: shape=(16384, 256), dtype=float32, numpy=\n",
      "array([[0., 1., 0., ..., 1., 0., 1.],\n",
      "       [0., 1., 1., ..., 0., 0., 1.],\n",
      "       [0., 0., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 0., 0., ..., 0., 1., 0.],\n",
      "       [1., 0., 0., ..., 0., 1., 0.],\n",
      "       [1., 1., 0., ..., 0., 1., 1.]], dtype=float32)>, <tf.Tensor: shape=(256, 256), dtype=float32, numpy=\n",
      "array([[1., 1., 1., ..., 1., 1., 0.],\n",
      "       [1., 1., 1., ..., 0., 1., 0.],\n",
      "       [1., 0., 0., ..., 1., 0., 1.],\n",
      "       ...,\n",
      "       [1., 1., 0., ..., 0., 1., 0.],\n",
      "       [0., 1., 1., ..., 0., 0., 1.],\n",
      "       [0., 1., 1., ..., 1., 1., 0.]], dtype=float32)>, <tf.Tensor: shape=(256, 10), dtype=float32, numpy=\n",
      "array([[1., 0., 0., ..., 1., 0., 1.],\n",
      "       [1., 0., 1., ..., 0., 0., 1.],\n",
      "       [1., 0., 0., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [0., 0., 1., ..., 1., 1., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       [1., 1., 1., ..., 0., 1., 0.]], dtype=float32)>]\n",
      "pruning_rates:  [0.5004258155822754, 0.503662109375, 0.49296874999999996]\n",
      "[<tf.Variable 'masked_dense_9/mask:0' shape=(16384, 256) dtype=float32, numpy=\n",
      "array([[-0.16769671,  0.32276893, -0.15153646, ...,  0.09651709,\n",
      "        -0.5385692 ,  0.7388115 ],\n",
      "       [-0.2606964 ,  0.6790328 ,  0.33383775, ..., -0.37051773,\n",
      "        -0.01103806,  0.7385254 ],\n",
      "       [-0.52405214, -0.0695796 ,  0.5173633 , ...,  0.879092  ,\n",
      "         0.78834915,  0.24582624],\n",
      "       ...,\n",
      "       [ 0.6201637 , -0.32966518, -0.2605245 , ..., -0.44737768,\n",
      "         0.4117818 , -0.17267728],\n",
      "       [ 0.5161705 , -0.21533084, -0.96499133, ..., -0.84137225,\n",
      "         0.2649567 , -0.02497578],\n",
      "       [ 0.3003931 ,  0.21389413, -0.06469297, ..., -0.7746711 ,\n",
      "         0.6036601 ,  0.06151533]], dtype=float32)>, <tf.Variable 'masked_dense_10/mask:0' shape=(256, 256) dtype=float32, numpy=\n",
      "array([[ 0.99110365,  0.49302268,  0.25353622, ...,  0.4024465 ,\n",
      "         0.86300635, -0.29617524],\n",
      "       [ 0.44394565,  0.78445053,  0.18844366, ..., -0.6849911 ,\n",
      "         0.66639495, -0.55938745],\n",
      "       [ 0.936712  , -0.42601442, -0.1008997 , ...,  0.7235527 ,\n",
      "        -0.8329487 ,  0.4577341 ],\n",
      "       ...,\n",
      "       [ 0.94095564,  0.35209584, -0.16997004, ..., -0.23656487,\n",
      "         0.17381835, -0.46345973],\n",
      "       [-0.96217513,  0.57883596,  0.6726358 , ..., -0.67818403,\n",
      "        -0.87713027,  0.35162497],\n",
      "       [-0.48331118,  0.6636536 ,  0.6246724 , ...,  0.76682377,\n",
      "         0.33307028, -0.6073289 ]], dtype=float32)>, <tf.Variable 'masked_dense_11/mask:0' shape=(256, 10) dtype=float32, numpy=\n",
      "array([[ 0.6802385 , -0.31566882, -0.62436986, ...,  0.46554422,\n",
      "        -0.72229314,  0.69384885],\n",
      "       [ 0.5450978 , -0.29220414,  0.10997224, ..., -0.57311463,\n",
      "        -0.08744979,  0.0216403 ],\n",
      "       [ 0.44412112, -0.82029605, -0.60668993, ...,  0.42050028,\n",
      "         0.01555204,  0.16507578],\n",
      "       ...,\n",
      "       [-0.3382361 , -0.506763  ,  0.7402952 , ...,  0.31500053,\n",
      "         0.09572721, -0.53084564],\n",
      "       [-0.05083132,  0.8928139 , -0.1665554 , ..., -0.21433973,\n",
      "        -0.993649  , -0.3003471 ],\n",
      "       [ 0.89363384,  0.34592652,  0.5137596 , ..., -0.6141715 ,\n",
      "         0.09741783, -0.03924131]], dtype=float32)>]\n",
      "Model: \"cnn2_model_masked_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           multiple                  1792      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           multiple                  36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " masked_dense_9 (MaskedDense  multiple                 8388864   \n",
      " )                                                               \n",
      "                                                                 \n",
      " masked_dense_10 (MaskedDens  multiple                 131328    \n",
      " e)                                                              \n",
      "                                                                 \n",
      " masked_dense_11 (MaskedDens  multiple                 5130      \n",
      " e)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,564,042\n",
      "Trainable params: 4,262,400\n",
      "Non-trainable params: 4,301,642\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['masked_dense_9/mask:0', 'masked_dense_10/mask:0', 'masked_dense_11/mask:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'masked_dense_9/mask:0' shape=(16384, 256) dtype=float32, numpy=\narray([[-0.16769671,  0.32276893, -0.15153646, ...,  0.09651709,\n        -0.5385692 ,  0.7388115 ],\n       [-0.2606964 ,  0.6790328 ,  0.33383775, ..., -0.37051773,\n        -0.01103806,  0.7385254 ],\n       [-0.52405214, -0.0695796 ,  0.5173633 , ...,  0.879092  ,\n         0.78834915,  0.24582624],\n       ...,\n       [ 0.6201637 , -0.32966518, -0.2605245 , ..., -0.44737768,\n         0.4117818 , -0.17267728],\n       [ 0.5161705 , -0.21533084, -0.96499133, ..., -0.84137225,\n         0.2649567 , -0.02497578],\n       [ 0.3003931 ,  0.21389413, -0.06469297, ..., -0.7746711 ,\n         0.6036601 ,  0.06151533]], dtype=float32)>), (None, <tf.Variable 'masked_dense_10/mask:0' shape=(256, 256) dtype=float32, numpy=\narray([[ 0.99110365,  0.49302268,  0.25353622, ...,  0.4024465 ,\n         0.86300635, -0.29617524],\n       [ 0.44394565,  0.78445053,  0.18844366, ..., -0.6849911 ,\n         0.66639495, -0.55938745],\n       [ 0.936712  , -0.42601442, -0.1008997 , ...,  0.7235527 ,\n        -0.8329487 ,  0.4577341 ],\n       ...,\n       [ 0.94095564,  0.35209584, -0.16997004, ..., -0.23656487,\n         0.17381835, -0.46345973],\n       [-0.96217513,  0.57883596,  0.6726358 , ..., -0.67818403,\n        -0.87713027,  0.35162497],\n       [-0.48331118,  0.6636536 ,  0.6246724 , ...,  0.76682377,\n         0.33307028, -0.6073289 ]], dtype=float32)>), (None, <tf.Variable 'masked_dense_11/mask:0' shape=(256, 10) dtype=float32, numpy=\narray([[ 0.6802385 , -0.31566882, -0.62436986, ...,  0.46554422,\n        -0.72229314,  0.69384885],\n       [ 0.5450978 , -0.29220414,  0.10997224, ..., -0.57311463,\n        -0.08744979,  0.0216403 ],\n       [ 0.44412112, -0.82029605, -0.60668993, ...,  0.42050028,\n         0.01555204,  0.16507578],\n       ...,\n       [-0.3382361 , -0.506763  ,  0.7402952 , ...,  0.31500053,\n         0.09572721, -0.53084564],\n       [-0.05083132,  0.8928139 , -0.1665554 , ..., -0.21433973,\n        -0.993649  , -0.3003471 ],\n       [ 0.89363384,  0.34592652,  0.5137596 , ..., -0.6141715 ,\n         0.09741783, -0.03924131]], dtype=float32)>)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m---> 14\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m plot_losses(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIFAR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTestSuperMaskOtimization\u001b[39m\u001b[38;5;124m\"\u001b[39m, losses,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN Loss and Accuracy for supermask model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mtrain_mask\u001b[1;34m(train, test, model, num_epochs)\u001b[0m\n\u001b[0;32m     27\u001b[0m         train_accuracy\u001b[38;5;241m.\u001b[39mupdate_state(tf\u001b[38;5;241m.\u001b[39margmax(t,\u001b[38;5;241m1\u001b[39m), tf\u001b[38;5;241m.\u001b[39margmax(pred,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     28\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m---> 29\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# test step\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, t \u001b[38;5;129;01min\u001b[39;00m test:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:689\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, grads_and_vars, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, experimental_aggregate_gradients\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    650\u001b[0m ):\n\u001b[0;32m    651\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply gradients to variables.\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \n\u001b[0;32m    653\u001b[0m \u001b[38;5;124;03m    This is the second part of `minimize()`. It returns an `Operation` that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;124;03m      RuntimeError: If called in a cross-replica context.\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m     var_list \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m (_, v) \u001b[38;5;129;01min\u001b[39;00m grads_and_vars]\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name):\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;66;03m# Create iteration if necessary.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\utils.py:77\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[0;32m     76\u001b[0m     variable \u001b[38;5;241m=\u001b[39m ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars],)\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `grads_and_vars` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads_and_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     80\u001b[0m     )\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vars_with_empty_grads:\n\u001b[0;32m     82\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m     83\u001b[0m         (\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients do not exist for variables \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m when minimizing the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m         ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vars_with_empty_grads]),\n\u001b[0;32m     89\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: (['masked_dense_9/mask:0', 'masked_dense_10/mask:0', 'masked_dense_11/mask:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'masked_dense_9/mask:0' shape=(16384, 256) dtype=float32, numpy=\narray([[-0.16769671,  0.32276893, -0.15153646, ...,  0.09651709,\n        -0.5385692 ,  0.7388115 ],\n       [-0.2606964 ,  0.6790328 ,  0.33383775, ..., -0.37051773,\n        -0.01103806,  0.7385254 ],\n       [-0.52405214, -0.0695796 ,  0.5173633 , ...,  0.879092  ,\n         0.78834915,  0.24582624],\n       ...,\n       [ 0.6201637 , -0.32966518, -0.2605245 , ..., -0.44737768,\n         0.4117818 , -0.17267728],\n       [ 0.5161705 , -0.21533084, -0.96499133, ..., -0.84137225,\n         0.2649567 , -0.02497578],\n       [ 0.3003931 ,  0.21389413, -0.06469297, ..., -0.7746711 ,\n         0.6036601 ,  0.06151533]], dtype=float32)>), (None, <tf.Variable 'masked_dense_10/mask:0' shape=(256, 256) dtype=float32, numpy=\narray([[ 0.99110365,  0.49302268,  0.25353622, ...,  0.4024465 ,\n         0.86300635, -0.29617524],\n       [ 0.44394565,  0.78445053,  0.18844366, ..., -0.6849911 ,\n         0.66639495, -0.55938745],\n       [ 0.936712  , -0.42601442, -0.1008997 , ...,  0.7235527 ,\n        -0.8329487 ,  0.4577341 ],\n       ...,\n       [ 0.94095564,  0.35209584, -0.16997004, ..., -0.23656487,\n         0.17381835, -0.46345973],\n       [-0.96217513,  0.57883596,  0.6726358 , ..., -0.67818403,\n        -0.87713027,  0.35162497],\n       [-0.48331118,  0.6636536 ,  0.6246724 , ...,  0.76682377,\n         0.33307028, -0.6073289 ]], dtype=float32)>), (None, <tf.Variable 'masked_dense_11/mask:0' shape=(256, 10) dtype=float32, numpy=\narray([[ 0.6802385 , -0.31566882, -0.62436986, ...,  0.46554422,\n        -0.72229314,  0.69384885],\n       [ 0.5450978 , -0.29220414,  0.10997224, ..., -0.57311463,\n        -0.08744979,  0.0216403 ],\n       [ 0.44412112, -0.82029605, -0.60668993, ...,  0.42050028,\n         0.01555204,  0.16507578],\n       ...,\n       [-0.3382361 , -0.506763  ,  0.7402952 , ...,  0.31500053,\n         0.09572721, -0.53084564],\n       [-0.05083132,  0.8928139 , -0.1665554 , ..., -0.21433973,\n        -0.993649  , -0.3003471 ],\n       [ 0.89363384,  0.34592652,  0.5137596 , ..., -0.6141715 ,\n         0.09741783, -0.03924131]], dtype=float32)>))."
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_and_prep_dataset(\"CIFAR\", batch_size=60, shuffle_size=512)\n",
    "\n",
    "model = CNN2ModelMasked()\n",
    "model(list(train_dataset)[0][0])\n",
    "initial_weights = model.get_weights()\n",
    "initial_mask = model.get_masks()\n",
    "initial_b_mask = model.get_binary_masks()\n",
    "print(initial_mask)\n",
    "print(initial_b_mask)\n",
    "print(\"pruning_rates: \", get_pruning_rates(initial_b_mask))\n",
    "print(model.trainable_variables)\n",
    "model.summary()\n",
    "\n",
    "losses = train_mask(train_dataset, test_dataset, model)\n",
    "plot_losses(\"CIFAR\", \"TestSuperMaskOtimization\", losses,\"CNN Loss and Accuracy for supermask model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cf268",
   "metadata": {},
   "source": [
    "debugging to do:\n",
    "- check paper for optimizer\n",
    "- make the call function simpler\n",
    "- research other examples of unusual trainable parameters in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f0c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
