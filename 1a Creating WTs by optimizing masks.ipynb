{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3d387b",
   "metadata": {},
   "source": [
    "# Optimizing Masks to create WTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fed2c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 10:59:36.152240: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries and the cnn architecture I defined\n",
    "\n",
    "from cnn_architecture import CNN2Model\n",
    "from utils import *\n",
    "from load_datasets import load_and_prep_dataset\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "import copy\n",
    "\n",
    "# all the extra stuff for supermasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08c7e8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279914f",
   "metadata": {},
   "source": [
    "all the variables i have to check their meaning:\n",
    "- use bias\n",
    "- dynamik scaling\n",
    "- sigmoid bias\n",
    "- use learning phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a9fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedDense(tf.keras.layers.Dense):\n",
    "    \n",
    "    # untrainable normal Dense layer\n",
    "    # trainable mask, that is sigmoided (maybe squished) and then multiplied to Dense\n",
    "    \n",
    "    def __init__(self, units,*args, **kwargs):\n",
    "        super(MaskedDense, self).__init__(units, *args, **kwargs)        \n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(MaskedDense, self).build(input_shape)\n",
    "        \n",
    "        # make bias and weights untrainable\n",
    "        self._trainable_weights.remove(self.kernel)\n",
    "        self._non_trainable_weights.append(self.kernel)\n",
    "        self._trainable_weights.remove(self.bias)\n",
    "        self._non_trainable_weights.append(self.bias)\n",
    "        \n",
    "        # create mask and make it trainable\n",
    "        mask_init = tf.random.uniform(shape=self.kernel.shape,minval=-1, maxval=1, seed=None)\n",
    "        self.kernel_mask = tf.Variable(initial_value=mask_init,\n",
    "                                        trainable=True,\n",
    "                                        validate_shape=True,\n",
    "                                        name='mask',\n",
    "                                        dtype=self.dtype,\n",
    "                                        shape=self.kernel.shape)\n",
    "        self._trainable_weights.append(self.kernel_mask)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "\n",
    "        #effective_mask = tf.cast(tfp.distributions.Bernoulli(probs=tf.nn.sigmoid(self.kernel_mask)).sample(), dtype=tf.float32)\n",
    "        effective_mask = tf.math.round(self.kernel_mask * 0.5 + 0.5)\n",
    "        effective_kernel = tf.math.multiply(self.kernel, effective_mask)\n",
    "\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "        outputs = tf.linalg.matmul(inputs, effective_kernel)\n",
    "        outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "        output =  self.activation(outputs)\n",
    "        #output = self.activation(tf.matmul(inputs, tf.math.multiply(self.kernel, tf.math.round(tf.nn.sigmoid(self.kernel_mask)))))\n",
    "        return output\n",
    "    \n",
    "    def get_mask(self):\n",
    "        return  tf.nn.sigmoid(self.kernel_mask)\n",
    "    \n",
    "    def get_binary_mask(self):\n",
    "        return tf.math.round(tf.nn.sigmoid(self.kernel_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "517a819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2ModelMasked(tf.keras.Model):\n",
    "    \n",
    "    # basic\n",
    "    def __init__(self):\n",
    "        super(CNN2ModelMasked, self).__init__()\n",
    "        \n",
    "        # set biases to a value that is not exactly 0.0, so they don't get handled like pruned values\n",
    "        self.bias_in = tf.keras.initializers.Constant(value=0.0000000001)\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=64, kernel_size=3,activation=\"relu\", padding=\"same\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batchsize,32,32,64]\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3,activation=\"relu\", padding=\"same\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batchsize,32,32,64]\n",
    "        self.maxpool = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2, 2),input_shape=(32, 32, 64)) # [batchsize,16,16,64]\n",
    "        self.flatten = tf.keras.layers.Flatten() # [batch_size,16384]\n",
    "        self.dense1 = MaskedDense(256, activation=\"relu\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batch_size,256]\n",
    "        self.dense2 = MaskedDense(256, activation=\"relu\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batch_size,256]\n",
    "        self.dense3 = MaskedDense(10, activation=\"softmax\",kernel_initializer='glorot_uniform', bias_initializer=self.bias_in) # [batch_size,256]\n",
    "        \n",
    "        # Making the weights of the conv layers untrainable\n",
    "        self.conv1.trainable = False\n",
    "        self.conv2.trainable = False\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # adjust the dense layers to be multiplayed with trainable mask (which gets assigned binary values for this step)\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    def get_masks(self):\n",
    "        return [self.dense1.get_mask(), self.dense2.get_mask(), self.dense3.get_mask()]\n",
    "        \n",
    "    def get_binary_masks(self):\n",
    "        return [self.dense1.get_binary_mask(), self.dense2.get_binary_mask(), self.dense3.get_binary_mask()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42718cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified train loop to also work with sparse networks (such that pruned weights remain frozen at 0.0)\n",
    "\n",
    "def train_mask(train, test, model, num_epochs=5):\n",
    "    \n",
    "    # hyperparameters\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    loss_function= tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    # initializing training statistics\n",
    "    train_accuracy = tf.keras.metrics.Accuracy(name='test_accuracy')\n",
    "    test_accuracy = tf.keras.metrics.Accuracy(name='train_accuracy')\n",
    "    train_losses = tf.keras.metrics.CategoricalCrossentropy(name='train_losses')\n",
    "    test_losses = tf.keras.metrics.CategoricalCrossentropy(name='test_losses')\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    train_l =[]\n",
    "    test_l = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), leave=False, desc=\"training epochs\"):\n",
    "        \n",
    "        #train step\n",
    "        for x, t in train:\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = model(x)\n",
    "                loss = loss_function(t, pred)\n",
    "                train_losses.update_state(t, pred)\n",
    "                train_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "        # test step\n",
    "        for x, t in test:\n",
    "            pred = model(x)\n",
    "            test_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "            test_losses.update_state(t, pred)\n",
    "        \n",
    "        # updataing training statistics\n",
    "        train_acc.append(train_accuracy.result().numpy())\n",
    "        test_acc.append(test_accuracy.result().numpy())\n",
    "        train_l.append(train_losses.result().numpy())\n",
    "        test_l.append(test_losses.result().numpy())\n",
    "        train_accuracy.reset_state()\n",
    "        test_accuracy.reset_state()\n",
    "        train_losses.reset_state()\n",
    "        test_losses.reset_state()\n",
    "        \n",
    "    # collecting losses in a dictionary\n",
    "    losses = { \"test loss\":test_l , \"training loss\":train_l , \"test accuracy\":test_acc , \"training accuracy\":train_acc}\n",
    "    \n",
    "    return  losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cc4a3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 10:52:18.703185: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2024-05-27 10:52:18.703558: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_3}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(16384, 256), dtype=float32, numpy=\n",
      "array([[0.7216107 , 0.47060272, 0.27487886, ..., 0.29052103, 0.48986143,\n",
      "        0.71482235],\n",
      "       [0.71817756, 0.5249277 , 0.6822148 , ..., 0.57077175, 0.6219044 ,\n",
      "        0.35620093],\n",
      "       [0.719473  , 0.5505622 , 0.5259809 , ..., 0.39946264, 0.5493305 ,\n",
      "        0.7186712 ],\n",
      "       ...,\n",
      "       [0.4860575 , 0.6810141 , 0.5011805 , ..., 0.617953  , 0.51898515,\n",
      "        0.4359818 ],\n",
      "       [0.62306225, 0.71918696, 0.63187224, ..., 0.5278704 , 0.53900576,\n",
      "        0.31025913],\n",
      "       [0.69953924, 0.30390292, 0.49955446, ..., 0.34767592, 0.5113065 ,\n",
      "        0.7180098 ]], dtype=float32)>, <tf.Tensor: shape=(256, 256), dtype=float32, numpy=\n",
      "array([[0.50213325, 0.55181617, 0.61439615, ..., 0.35733804, 0.7217588 ,\n",
      "        0.66565555],\n",
      "       [0.702278  , 0.31642437, 0.32612178, ..., 0.30883265, 0.33688396,\n",
      "        0.70692664],\n",
      "       [0.5550536 , 0.5511965 , 0.30602065, ..., 0.42932206, 0.7255371 ,\n",
      "        0.61269784],\n",
      "       ...,\n",
      "       [0.6728859 , 0.5852464 , 0.37039712, ..., 0.35416406, 0.7158597 ,\n",
      "        0.38404655],\n",
      "       [0.6697646 , 0.6665942 , 0.6453931 , ..., 0.3023809 , 0.32162648,\n",
      "        0.4805376 ],\n",
      "       [0.41006753, 0.3026331 , 0.33152288, ..., 0.7294063 , 0.7142818 ,\n",
      "        0.503382  ]], dtype=float32)>, <tf.Tensor: shape=(256, 10), dtype=float32, numpy=\n",
      "array([[0.607187  , 0.57824385, 0.52447325, ..., 0.5711059 , 0.38689074,\n",
      "        0.5441903 ],\n",
      "       [0.7003617 , 0.6526867 , 0.3813189 , ..., 0.42231193, 0.38297424,\n",
      "        0.70915234],\n",
      "       [0.5046541 , 0.3510011 , 0.3242065 , ..., 0.2799901 , 0.5583365 ,\n",
      "        0.7207919 ],\n",
      "       ...,\n",
      "       [0.4054681 , 0.44158766, 0.6621552 , ..., 0.5741742 , 0.69577336,\n",
      "        0.6358271 ],\n",
      "       [0.6826274 , 0.51046574, 0.5888736 , ..., 0.47070885, 0.6591343 ,\n",
      "        0.5238405 ],\n",
      "       [0.6616465 , 0.6433758 , 0.714487  , ..., 0.7012136 , 0.7275921 ,\n",
      "        0.5730702 ]], dtype=float32)>]\n",
      "[<tf.Tensor: shape=(16384, 256), dtype=float32, numpy=\n",
      "array([[1., 0., 0., ..., 0., 0., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 0.],\n",
      "       [1., 1., 1., ..., 0., 1., 1.],\n",
      "       ...,\n",
      "       [0., 1., 1., ..., 1., 1., 0.],\n",
      "       [1., 1., 1., ..., 1., 1., 0.],\n",
      "       [1., 0., 0., ..., 0., 1., 1.]], dtype=float32)>, <tf.Tensor: shape=(256, 256), dtype=float32, numpy=\n",
      "array([[1., 1., 1., ..., 0., 1., 1.],\n",
      "       [1., 0., 0., ..., 0., 0., 1.],\n",
      "       [1., 1., 0., ..., 0., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 0., ..., 0., 1., 0.],\n",
      "       [1., 1., 1., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 1., 1.]], dtype=float32)>, <tf.Tensor: shape=(256, 10), dtype=float32, numpy=\n",
      "array([[1., 1., 1., ..., 1., 0., 1.],\n",
      "       [1., 1., 0., ..., 0., 0., 1.],\n",
      "       [1., 0., 0., ..., 0., 1., 1.],\n",
      "       ...,\n",
      "       [0., 0., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 0., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>]\n",
      "pruning_rates:  [0.5002567768096924, 0.50164794921875, 0.505078125]\n",
      "[<tf.Variable 'masked_dense_3/mask:0' shape=(16384, 256) dtype=float32, numpy=\n",
      "array([[ 0.95246553, -0.1177249 , -0.9700084 , ..., -0.8928549 ,\n",
      "        -0.04055977,  0.9189217 ],\n",
      "       [ 0.9354396 ,  0.0997932 ,  0.76396894, ...,  0.28500056,\n",
      "         0.49763918, -0.59189177],\n",
      "       [ 0.941849  ,  0.20294261,  0.10401726, ..., -0.4077046 ,\n",
      "         0.19796586,  0.9378798 ],\n",
      "       ...,\n",
      "       [-0.05578446,  0.7584362 ,  0.00472212, ...,  0.48086882,\n",
      "         0.07597685, -0.25748587],\n",
      "       [ 0.50256634,  0.9404321 ,  0.5402572 , ...,  0.1115973 ,\n",
      "         0.15634084, -0.798908  ],\n",
      "       [ 0.8451047 , -0.8287809 , -0.00178218, ..., -0.6292708 ,\n",
      "         0.04523373,  0.9346111 ]], dtype=float32)>, <tf.Variable 'masked_dense_4/mask:0' shape=(256, 256) dtype=float32, numpy=\n",
      "array([[ 0.008533  ,  0.20801139,  0.4658296 , ..., -0.5869367 ,\n",
      "         0.9532025 ,  0.68860054],\n",
      "       [ 0.8581693 , -0.77025294, -0.7257786 , ..., -0.8055825 ,\n",
      "        -0.6772115 ,  0.8805039 ],\n",
      "       [ 0.22111082,  0.20550632, -0.8187895 , ..., -0.28461766,\n",
      "         0.9720962 ,  0.45866656],\n",
      "       ...,\n",
      "       [ 0.72126675,  0.34434843, -0.5305135 , ..., -0.60078526,\n",
      "         0.924016  , -0.47240758],\n",
      "       [ 0.7071204 ,  0.692821  ,  0.59884953, ..., -0.8359859 ,\n",
      "        -0.74630713, -0.07788897],\n",
      "       [-0.36368632, -0.83479047, -0.7013054 , ...,  0.99161243,\n",
      "         0.91627145,  0.01352811]], dtype=float32)>, <tf.Variable 'masked_dense_5/mask:0' shape=(256, 10) dtype=float32, numpy=\n",
      "array([[ 0.435503  ,  0.3155682 ,  0.0979712 , ...,  0.2863648 ,\n",
      "        -0.46040082,  0.17722368],\n",
      "       [ 0.84902096,  0.6308701 , -0.48395395, ..., -0.31328988,\n",
      "        -0.47694302,  0.89127064],\n",
      "       [ 0.01861715, -0.61464167, -0.73450685, ..., -0.9445107 ,\n",
      "         0.23441362,  0.94839287],\n",
      "       ...,\n",
      "       [-0.38273215, -0.23472118,  0.67291355, ...,  0.2989025 ,\n",
      "         0.82725096,  0.55729795],\n",
      "       [ 0.7658727 ,  0.04186916,  0.3593111 , ..., -0.11729884,\n",
      "         0.65943885,  0.09543443],\n",
      "       [ 0.67064023,  0.5900464 ,  0.91727734, ...,  0.8530836 ,\n",
      "         0.98244   ,  0.29438877]], dtype=float32)>]\n",
      "Model: \"cnn2_model_masked_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           multiple                  1792      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           multiple                  36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " masked_dense_3 (MaskedDense  multiple                 8388864   \n",
      " )                                                               \n",
      "                                                                 \n",
      " masked_dense_4 (MaskedDense  multiple                 131328    \n",
      " )                                                               \n",
      "                                                                 \n",
      " masked_dense_5 (MaskedDense  multiple                 5130      \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,564,042\n",
      "Trainable params: 4,262,400\n",
      "Non-trainable params: 4,301,642\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epochs:   0%|          | 0/5 [00:00<?, ?it/s]2024-05-27 10:55:22.947659: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-05-27 10:55:22.948633: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "                                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m---> 14\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m plot_losses(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIFAR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTestSuperMaskOptimization\u001b[39m\u001b[38;5;124m\"\u001b[39m, losses,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN Loss and Accuracy for supermask model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m, in \u001b[0;36mtrain_mask\u001b[0;34m(train, test, model, num_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         train_losses\u001b[38;5;241m.\u001b[39mupdate_state(t, pred)\n\u001b[1;32m     27\u001b[0m         train_accuracy\u001b[38;5;241m.\u001b[39mupdate_state(tf\u001b[38;5;241m.\u001b[39margmax(t,\u001b[38;5;241m1\u001b[39m), tf\u001b[38;5;241m.\u001b[39margmax(pred,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 28\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# test step\u001b[39;00m\n",
      "File \u001b[0;32m/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:1063\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1057\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1058\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1059\u001b[0m           output_gradients))\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1061\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1063\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1072\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1085\u001b[0m, in \u001b[0;36m_TapeGradientFunctions._wrap_backward_function.<locals>._backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m input_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m backward_function_inputs:\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremapped_captures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_and_prep_dataset(\"CIFAR\", batch_size=60, shuffle_size=512)\n",
    "\n",
    "model = CNN2ModelMasked()\n",
    "model(list(train_dataset)[0][0])\n",
    "initial_weights = model.get_weights()\n",
    "initial_mask = model.get_masks()\n",
    "initial_b_mask = model.get_binary_masks()\n",
    "print(initial_mask)\n",
    "print(initial_b_mask)\n",
    "print(\"pruning_rates: \", get_pruning_rates(initial_b_mask))\n",
    "print(model.trainable_variables)\n",
    "model.summary()\n",
    "\n",
    "losses = train_mask(train_dataset, test_dataset, model)\n",
    "plot_losses(\"CIFAR\", \"TestSuperMaskOptimization\", losses,\"CNN Loss and Accuracy for supermask model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cf268",
   "metadata": {},
   "source": [
    "debugging to do:\n",
    "- check paper for optimizer\n",
    "- make the call function simpler\n",
    "- research other examples of unusual trainable parameters in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27f0c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
