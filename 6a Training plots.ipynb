{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f26ae5bb",
   "metadata": {},
   "source": [
    "# Training Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd8090f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 09:37:43.342508: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/miniconda3/envs/thesis_gpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries and the cnn architecture I defined\n",
    "\n",
    "from cnn_architecture import CNN2Model\n",
    "from utils import *\n",
    "from load_datasets import load_and_prep_dataset\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "import copy\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5460b",
   "metadata": {},
   "source": [
    "### Train Loop and IMP loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c60f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified train loop to also work with sparse networks (such that pruned weights remain frozen at 0.0)\n",
    "\n",
    "def train_loop_sparse(train, test, model, num_epochs=5):\n",
    "    \n",
    "    # hyperparameters\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "    loss_function= tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "    # initializing training statistics\n",
    "    train_accuracy = tf.keras.metrics.Accuracy(name='test_accuracy')\n",
    "    test_accuracy = tf.keras.metrics.Accuracy(name='train_accuracy')\n",
    "    train_losses = tf.keras.metrics.CategoricalCrossentropy(name='train_losses')\n",
    "    test_losses = tf.keras.metrics.CategoricalCrossentropy(name='test_losses')\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    train_l =[]\n",
    "    test_l = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), leave=False, desc=\"training epochs\"):\n",
    "        \n",
    "        #train step\n",
    "        for x, t in train:\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = model(x)\n",
    "                loss = loss_function(t, pred)\n",
    "                train_losses.update_state(t, pred)\n",
    "                train_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            \n",
    "            # set gradients to zero for all pruned weights\n",
    "            new_gradients = []\n",
    "            for gradient_matrix, variables in zip(gradients, model.trainable_variables):\n",
    "                gradient_matrix = tf.where(variables == 0.0, 0.0, gradient_matrix)\n",
    "                new_gradients.append(gradient_matrix)\n",
    "            optimizer.apply_gradients(zip(new_gradients, model.trainable_variables))\n",
    "            \n",
    "        # test step\n",
    "        for x, t in test:\n",
    "            pred = model(x)\n",
    "            test_accuracy.update_state(tf.argmax(t,1), tf.argmax(pred,1))\n",
    "            test_losses.update_state(t, pred)\n",
    "        \n",
    "        # updataing training statistics\n",
    "        train_acc.append(train_accuracy.result().numpy())\n",
    "        test_acc.append(test_accuracy.result().numpy())\n",
    "        train_l.append(train_losses.result().numpy())\n",
    "        test_l.append(test_losses.result().numpy())\n",
    "        train_accuracy.reset_state()\n",
    "        test_accuracy.reset_state()\n",
    "        train_losses.reset_state()\n",
    "        test_losses.reset_state()\n",
    "        \n",
    "    # collecting losses in a dictionary\n",
    "    losses = {\"test loss\":test_l , \"training loss\":train_l , \"test accuracy\":test_acc , \"training accuracy\":train_acc}\n",
    "    losses = pd.DataFrame.from_dict(losses, orient='columns')\n",
    "    \n",
    "    return  losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928067b",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- Winning Tickets\n",
    "- Random Tickets\n",
    "- Control Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5b6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_control(train_dataset, test_dataset, nr):\n",
    "\n",
    "    all_losses = pd.DataFrame()\n",
    "\n",
    "    for i in range(nr):\n",
    "\n",
    "        control_model = CNN2Model()\n",
    "        control_losses = train_loop_sparse(train_dataset, test_dataset, control_model)\n",
    "        control_losses[\"model_nr\"] = i\n",
    "        control_losses[\"Sparsity\"] = \"original\"\n",
    "        control_losses[\"epochs\"] = control_losses.index\n",
    "\n",
    "        all_losses = pd.concat([all_losses, control_losses], axis=0)\n",
    "    \n",
    "    return all_losses\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa04f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wts(train_dataset, test_dataset, dataset, nrs):\n",
    "\n",
    "    all_losses = pd.DataFrame()\n",
    "\n",
    "    for i in nrs:\n",
    "\n",
    "        wt_model = CNN2Model()\n",
    "        wt_model(list(train_dataset)[0][0])\n",
    "        wt_model.load_weights(f\"1b WTs/h5_format/WT_{dataset}_IMP_{i}.h5\")\n",
    "\n",
    "\n",
    "        wt_losses = train_loop_sparse(train_dataset, test_dataset, wt_model)\n",
    "        wt_losses[\"model_nr\"] = i\n",
    "        wt_losses[\"Sparsity\"] = \"WT\"\n",
    "        wt_losses[\"epochs\"] = wt_losses.index\n",
    "\n",
    "        all_losses = pd.concat([all_losses, wt_losses], axis=0)\n",
    "    \n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760fe4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_sparse(train_dataset, test_dataset, nrs):\n",
    "\n",
    "    all_losses = pd.DataFrame()\n",
    "\n",
    "    for i in nrs:\n",
    "\n",
    "        random_sparse_model = CNN2Model()\n",
    "        random_sparse_model(list(train_dataset)[0][0])\n",
    "        random_sparse_model.load_weights(f\"1b WTs/h5_format/RSN_{i}.h5\")\n",
    "\n",
    "\n",
    "        random_sparse_losses = train_loop_sparse(train_dataset, test_dataset, random_sparse_model)\n",
    "        random_sparse_losses[\"model_nr\"] = i\n",
    "        random_sparse_losses[\"Sparsity\"] = \"RSN\"\n",
    "        random_sparse_losses[\"epochs\"] = random_sparse_losses.index\n",
    "\n",
    "        all_losses = pd.concat([all_losses, random_sparse_losses], axis=0)\n",
    "    \n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdec8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the rsn random and change the sparsity column and save\n",
    "random_sparse_losses = pd.read_csv('6b Training statistics/CIFAR_RSN_training_stats.csv')\n",
    "random_sparse_losses[\"Sparsity\"] = \"RSN\"\n",
    "random_sparse_losses.to_csv('6b Training statistics/CIFAR_RSN_training_stats.csv', index=False)\n",
    "\n",
    "wt_losses = pd.read_csv('6b Training statistics/CIFAR_WT_training_stats.csv')\n",
    "wt_losses[\"Sparsity\"] = \"WT\"\n",
    "wt_losses.to_csv(f'6b Training statistics/CIFAR_WT_training_stats.csv', index=False)\n",
    "\n",
    "\n",
    "control_losses = pd.read_csv(\"6b Training statistics/CIFAR_original_training_stats.csv\")\n",
    "\n",
    "coll_stats = pd.concat([control_losses,wt_losses,random_sparse_losses], axis =0)\n",
    "coll_stats.to_csv(f'6b Training statistics/CIFAR_collective_training_stats.csv', index=False)\n",
    "# reload all three training statistics and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058f8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_statistics(dataset):\n",
    "    \n",
    "    nr = 15\n",
    "\n",
    "    train_dataset, test_dataset = load_and_prep_dataset(dataset, batch_size=60, shuffle_size=512)\n",
    "\n",
    "    control_stats = train_control(train_dataset, test_dataset, nr)\n",
    "    control_stats[\"dataset\"]=dataset\n",
    "    #display(control_stats)\n",
    "    control_stats.to_csv(f'6b Training statistics/{dataset}_original_training_stats.csv', index=False)\n",
    "\n",
    "    wts_stats = train_wts(train_dataset, test_dataset, dataset, range(nr))\n",
    "    wts_stats[\"dataset\"]=dataset\n",
    "    #display(wts_stats)\n",
    "    wts_stats.to_csv(f'6b Training statistics/{dataset}_WT_training_stats.csv', index=False)\n",
    "\n",
    "    random_stats = train_random_sparse(train_dataset, test_dataset, range(nr))\n",
    "    random_stats[\"dataset\"]=dataset\n",
    "    #display(random_stats)\n",
    "    random_stats.to_csv(f'6b Training statistics/{dataset}_RSN_training_stats.csv', index=False)\n",
    "\n",
    "    coll_stats = pd.concat([control_stats,wts_stats,random_stats], axis =0)\n",
    "    coll_stats.to_csv(f'6b Training statistics/{dataset}_collective_training_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdd2f690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89326 files belonging to 10 classes.\n",
      "Using 80394 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 09:37:54.597032: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 09:37:54.646783: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 09:37:54.646971: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 09:37:54.647755: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 09:37:54.647931: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 09:37:54.648082: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 09:37:54.712964: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 09:37:54.713199: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 09:37:54.713361: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 09:37:54.713474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3059 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89326 files belonging to 10 classes.\n",
      "Using 8932 files for validation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api._tf_keras.keras.layers' has no attribute 'experimental'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#get_train_statistics(\"CIFAR\")\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mget_train_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCINIC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m get_train_statistics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVHN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mget_train_statistics\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_train_statistics\u001b[39m(dataset):\n\u001b[1;32m      3\u001b[0m     nr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m----> 5\u001b[0m     train_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_prep_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     control_stats \u001b[38;5;241m=\u001b[39m train_control(train_dataset, test_dataset, nr)\n\u001b[1;32m      8\u001b[0m     control_stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mdataset\n",
      "File \u001b[0;32m/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/ThesisLTH/load_datasets.py:145\u001b[0m, in \u001b[0;36mload_and_prep_dataset\u001b[0;34m(dataset, batch_size, shuffle_size)\u001b[0m\n\u001b[1;32m    143\u001b[0m     train_dataset, test_dataset \u001b[38;5;241m=\u001b[39m load_and_prep_cifar(batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle_size\u001b[38;5;241m=\u001b[39mshuffle_size)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCINIC\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 145\u001b[0m     train_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_prep_cinic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVHN\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    147\u001b[0m     train_dataset, test_dataset \u001b[38;5;241m=\u001b[39m load_and_prep_svhn(batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle_size\u001b[38;5;241m=\u001b[39mshuffle_size)\n",
      "File \u001b[0;32m/net/projects/scratch/summer/valid_until_31_January_2025/epetersen/ThesisLTH/load_datasets.py:122\u001b[0m, in \u001b[0;36mload_and_prep_cinic\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m    113\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[1;32m    114\u001b[0m     FOLDER_TRAIN_DATASET_CINIC,\n\u001b[1;32m    115\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m(img_rows, img_cols),\n\u001b[1;32m    119\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# normalize images\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m normalization_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mRescaling(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    123\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: (normalization_layer(x), y))\n\u001b[1;32m    124\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m val_ds\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: (normalization_layer(x), y))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.api._tf_keras.keras.layers' has no attribute 'experimental'"
     ]
    }
   ],
   "source": [
    "#get_train_statistics(\"CIFAR\")\n",
    "get_train_statistics(\"CINIC\")\n",
    "get_train_statistics(\"SVHN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
